{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepImpute For Saver Dataset\n",
    "\n",
    "Mouse brain single-cell RNA-seq dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 3529 cells and 200 genes\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/juank/Desktop/BCOM/Proyecto/deepimpute')\n",
    "\n",
    "from deepimpute.multinet import MultiNet\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset using pandas\n",
    "data = pd.read_csv('/Users/juank/Desktop/BCOM/Proyecto/deepimpute/data/linnarsson.csv',index_col=0)\n",
    "print('Working on {} cells and {} genes'.format(*data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1772071015_C02</th>\n",
       "      <th>1772071017_G12</th>\n",
       "      <th>1772071017_A05</th>\n",
       "      <th>1772071014_B06</th>\n",
       "      <th>1772067065_H06</th>\n",
       "      <th>1772071017_E02</th>\n",
       "      <th>1772067065_B07</th>\n",
       "      <th>1772067060_B09</th>\n",
       "      <th>1772071014_E04</th>\n",
       "      <th>1772071015_D04</th>\n",
       "      <th>...</th>\n",
       "      <th>1772067076_E04</th>\n",
       "      <th>1772066097_C06</th>\n",
       "      <th>1772067057_D12</th>\n",
       "      <th>1772066100_A05</th>\n",
       "      <th>1772071015_A11</th>\n",
       "      <th>1772071015_C09</th>\n",
       "      <th>1772062128_F10</th>\n",
       "      <th>1772071017_H06</th>\n",
       "      <th>1772071014_A12</th>\n",
       "      <th>1772063077_H05</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Atp1b2</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sub1</th>\n",
       "      <td>28</td>\n",
       "      <td>41</td>\n",
       "      <td>57</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>49</td>\n",
       "      <td>26</td>\n",
       "      <td>47</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>27</td>\n",
       "      <td>44</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ptprf</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cers5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rit2</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eif1ax</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rbbp7</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trappc2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rab9</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vamp7</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3529 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         1772071015_C02  1772071017_G12  1772071017_A05  1772071014_B06  \\\n",
       "Atp1b2                9               5               8               6   \n",
       "Sub1                 28              41              57              33   \n",
       "Ptprf                 0               0               4               1   \n",
       "Cers5                 0               0               7               1   \n",
       "Rit2                  2               6               3               1   \n",
       "...                 ...             ...             ...             ...   \n",
       "Eif1ax                1               1               2               1   \n",
       "Rbbp7                 1               2               4               4   \n",
       "Trappc2               1               1               2               1   \n",
       "Rab9                  7               1               1               3   \n",
       "Vamp7                 5               0               3               0   \n",
       "\n",
       "         1772067065_H06  1772071017_E02  1772067065_B07  1772067060_B09  \\\n",
       "Atp1b2                7               9               0               3   \n",
       "Sub1                 30              13              27              17   \n",
       "Ptprf                 5               2               0               4   \n",
       "Cers5                 0               1               2               0   \n",
       "Rit2                 12               2               2               2   \n",
       "...                 ...             ...             ...             ...   \n",
       "Eif1ax                1               1               0               0   \n",
       "Rbbp7                 6               4               6               8   \n",
       "Trappc2               2               0               0               0   \n",
       "Rab9                  0               0               2               0   \n",
       "Vamp7                 3               2               8               2   \n",
       "\n",
       "         1772071014_E04  1772071015_D04  ...  1772067076_E04  1772066097_C06  \\\n",
       "Atp1b2                6               4  ...               2               0   \n",
       "Sub1                 23              31  ...              49              26   \n",
       "Ptprf                 0              10  ...               1               1   \n",
       "Cers5                 0               0  ...               3               0   \n",
       "Rit2                 10               7  ...               2               9   \n",
       "...                 ...             ...  ...             ...             ...   \n",
       "Eif1ax                0               1  ...               2               5   \n",
       "Rbbp7                 3               4  ...               1               4   \n",
       "Trappc2               1               1  ...               5               6   \n",
       "Rab9                  0               0  ...               1               2   \n",
       "Vamp7                 3               3  ...               0               3   \n",
       "\n",
       "         1772067057_D12  1772066100_A05  1772071015_A11  1772071015_C09  \\\n",
       "Atp1b2                2               1               2               7   \n",
       "Sub1                 47              16              18              27   \n",
       "Ptprf                 3               0               1               7   \n",
       "Cers5                13               2               0               2   \n",
       "Rit2                  2               0               6               3   \n",
       "...                 ...             ...             ...             ...   \n",
       "Eif1ax                4               2               0               0   \n",
       "Rbbp7                 4               0               2               2   \n",
       "Trappc2               5               2               1               2   \n",
       "Rab9                  3               2               0               6   \n",
       "Vamp7                 0               0               0               0   \n",
       "\n",
       "         1772062128_F10  1772071017_H06  1772071014_A12  1772063077_H05  \n",
       "Atp1b2                1              10               3               1  \n",
       "Sub1                 44               5              23              18  \n",
       "Ptprf                 7               0               2               0  \n",
       "Cers5                 3               3               4               1  \n",
       "Rit2                  5              11               2               5  \n",
       "...                 ...             ...             ...             ...  \n",
       "Eif1ax                1               1               3               3  \n",
       "Rbbp7                 0               2               1               0  \n",
       "Trappc2               1               0               4               0  \n",
       "Rab9                  2               2               2               3  \n",
       "Vamp7                 2               1               2               1  \n",
       "\n",
       "[3529 rows x 200 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DeepImpute multinet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all the cores (12)\n"
     ]
    }
   ],
   "source": [
    "# Using default parameters\n",
    "multinet = MultiNet() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using custom parameters\n",
    "NN_params = {\n",
    "        'learning_rate': 1e-4,\n",
    "        'batch_size': 64,\n",
    "        'max_epochs': 200,\n",
    "        'ncores': 5,\n",
    "        'sub_outputdim': 512,\n",
    "        'architecture': [\n",
    "            {\"type\": \"dense\", \"activation\": \"relu\", \"neurons\": 200},\n",
    "            {\"type\": \"dropout\", \"activation\": \"dropout\", \"rate\": 0.3}]\n",
    "    }\n",
    "\n",
    "multinet = MultiNet(**NN_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset is 3529 cells (rows) and 200 genes (columns)\n",
      "First 3 rows and columns:\n",
      "        1772071015_C02  1772071017_G12  1772071017_A05\n",
      "Atp1b2               9               5               8\n",
      "Sub1                28              41              57\n",
      "Ptprf                0               0               4\n",
      "512 genes selected for imputation\n",
      "Net 0: 200 predictors, 512 targets\n",
      "Normalization\n",
      "Building network\n",
      "[{'type': 'dense', 'activation': 'relu', 'neurons': 200}, {'type': 'dropout', 'activation': 'dropout', 'rate': 0.3}]\n",
      "Fitting with 3529 cells\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juank/Desktop/BCOM/Proyecto/deepimpute/deepimpute/multinet.py:355: UserWarning: Warning: number of target genes lower than output dim. Consider lowering down the sub_outputdim parameter\n",
      "  warnings.warn('Warning: number of target genes lower than output dim. Consider lowering down the sub_outputdim parameter',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5542 - val_loss: 1.0937\n",
      "Epoch 2/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 1.3908 - val_loss: 0.5106\n",
      "Epoch 3/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8878 - val_loss: 0.4085\n",
      "Epoch 4/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 986us/step - loss: 0.7230 - val_loss: 0.3786\n",
      "Epoch 5/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6502 - val_loss: 0.3582\n",
      "Epoch 6/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5923 - val_loss: 0.3459\n",
      "Epoch 7/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5571 - val_loss: 0.3305\n",
      "Epoch 8/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5315 - val_loss: 0.3192\n",
      "Epoch 9/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5020 - val_loss: 0.3093\n",
      "Epoch 10/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4790 - val_loss: 0.3065\n",
      "Epoch 11/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4692 - val_loss: 0.3008\n",
      "Epoch 12/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4504 - val_loss: 0.2899\n",
      "Epoch 13/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4366 - val_loss: 0.2956\n",
      "Epoch 14/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4245 - val_loss: 0.2835\n",
      "Epoch 15/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4174 - val_loss: 0.2794\n",
      "Epoch 16/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4045 - val_loss: 0.2746\n",
      "Epoch 17/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4032 - val_loss: 0.2714\n",
      "Epoch 18/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3942 - val_loss: 0.2693\n",
      "Epoch 19/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3849 - val_loss: 0.2659\n",
      "Epoch 20/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3809 - val_loss: 0.2684\n",
      "Epoch 21/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3739 - val_loss: 0.2607\n",
      "Epoch 22/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3692 - val_loss: 0.2585\n",
      "Epoch 23/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3665 - val_loss: 0.2565\n",
      "Epoch 24/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3557 - val_loss: 0.2539\n",
      "Epoch 25/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3526 - val_loss: 0.2559\n",
      "Epoch 26/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3523 - val_loss: 0.2511\n",
      "Epoch 27/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3509 - val_loss: 0.2475\n",
      "Epoch 28/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3415 - val_loss: 0.2482\n",
      "Epoch 29/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3364 - val_loss: 0.2440\n",
      "Epoch 30/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3318 - val_loss: 0.2440\n",
      "Epoch 31/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3271 - val_loss: 0.2404\n",
      "Epoch 32/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3267 - val_loss: 0.2391\n",
      "Epoch 33/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3271 - val_loss: 0.2380\n",
      "Epoch 34/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3225 - val_loss: 0.2384\n",
      "Epoch 35/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3168 - val_loss: 0.2347\n",
      "Epoch 36/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3166 - val_loss: 0.2385\n",
      "Epoch 37/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3116 - val_loss: 0.2345\n",
      "Epoch 38/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3123 - val_loss: 0.2301\n",
      "Epoch 39/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3065 - val_loss: 0.2308\n",
      "Epoch 40/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3019 - val_loss: 0.2281\n",
      "Epoch 41/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2943 - val_loss: 0.2259\n",
      "Epoch 42/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2956 - val_loss: 0.2233\n",
      "Epoch 43/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2979 - val_loss: 0.2229\n",
      "Epoch 44/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2933 - val_loss: 0.2294\n",
      "Epoch 45/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2973 - val_loss: 0.2234\n",
      "Epoch 46/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2976 - val_loss: 0.2191\n",
      "Epoch 47/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2856 - val_loss: 0.2176\n",
      "Epoch 48/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2861 - val_loss: 0.2185\n",
      "Epoch 49/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2911 - val_loss: 0.2163\n",
      "Epoch 50/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2796 - val_loss: 0.2165\n",
      "Epoch 51/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2762 - val_loss: 0.2138\n",
      "Epoch 52/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2746 - val_loss: 0.2117\n",
      "Epoch 53/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2734 - val_loss: 0.2109\n",
      "Epoch 54/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2725 - val_loss: 0.2113\n",
      "Epoch 55/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2710 - val_loss: 0.2114\n",
      "Epoch 56/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2704 - val_loss: 0.2076\n",
      "Epoch 57/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2671 - val_loss: 0.2081\n",
      "Epoch 58/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2625 - val_loss: 0.2061\n",
      "Epoch 59/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2622 - val_loss: 0.2049\n",
      "Epoch 60/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2635 - val_loss: 0.2044\n",
      "Epoch 61/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2588 - val_loss: 0.2089\n",
      "Epoch 62/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2601 - val_loss: 0.2030\n",
      "Epoch 63/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2583 - val_loss: 0.2024\n",
      "Epoch 64/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2572 - val_loss: 0.2006\n",
      "Epoch 65/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2622 - val_loss: 0.2006\n",
      "Epoch 66/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2550 - val_loss: 0.2014\n",
      "Epoch 67/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2573 - val_loss: 0.1983\n",
      "Epoch 68/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2529 - val_loss: 0.2008\n",
      "Epoch 69/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2553 - val_loss: 0.1965\n",
      "Epoch 70/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2504 - val_loss: 0.1975\n",
      "Epoch 71/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2489 - val_loss: 0.1958\n",
      "Epoch 72/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2515 - val_loss: 0.1940\n",
      "Epoch 73/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2475 - val_loss: 0.2109\n",
      "Epoch 74/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2482 - val_loss: 0.1937\n",
      "Epoch 75/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2444 - val_loss: 0.1926\n",
      "Epoch 76/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2447 - val_loss: 0.1930\n",
      "Epoch 77/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2524 - val_loss: 0.1915\n",
      "Epoch 78/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2436 - val_loss: 0.1909\n",
      "Epoch 79/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2434 - val_loss: 0.1911\n",
      "Epoch 80/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2407 - val_loss: 0.1927\n",
      "Epoch 81/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2346 - val_loss: 0.1896\n",
      "Epoch 82/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2428 - val_loss: 0.1894\n",
      "Epoch 83/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2413 - val_loss: 0.1889\n",
      "Epoch 84/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2363 - val_loss: 0.1918\n",
      "Epoch 85/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2331 - val_loss: 0.1888\n",
      "Epoch 86/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2357 - val_loss: 0.1880\n",
      "Epoch 87/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2378 - val_loss: 0.1864\n",
      "Epoch 88/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2336 - val_loss: 0.1861\n",
      "Epoch 89/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2331 - val_loss: 0.1851\n",
      "Epoch 90/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2338 - val_loss: 0.1853\n",
      "Epoch 91/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2308 - val_loss: 0.1843\n",
      "Epoch 92/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2319 - val_loss: 0.1873\n",
      "Epoch 93/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2380 - val_loss: 0.1832\n",
      "Epoch 94/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2290 - val_loss: 0.1855\n",
      "Epoch 95/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2316 - val_loss: 0.1823\n",
      "Epoch 96/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2289 - val_loss: 0.1854\n",
      "Epoch 97/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2231 - val_loss: 0.1853\n",
      "Epoch 98/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2264 - val_loss: 0.1805\n",
      "Epoch 99/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2295 - val_loss: 0.1803\n",
      "Epoch 100/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2245 - val_loss: 0.1835\n",
      "Epoch 101/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2304 - val_loss: 0.1797\n",
      "Epoch 102/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2256 - val_loss: 0.1816\n",
      "Epoch 103/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2230 - val_loss: 0.1788\n",
      "Epoch 104/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2239 - val_loss: 0.1797\n",
      "Epoch 105/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2230 - val_loss: 0.1802\n",
      "Epoch 106/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2257 - val_loss: 0.1789\n",
      "Epoch 107/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2251 - val_loss: 0.1771\n",
      "Epoch 108/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2217 - val_loss: 0.1779\n",
      "Epoch 109/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2204 - val_loss: 0.1763\n",
      "Epoch 110/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2211 - val_loss: 0.1814\n",
      "Epoch 111/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2212 - val_loss: 0.1776\n",
      "Epoch 112/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 999us/step - loss: 0.2239 - val_loss: 0.1758\n",
      "Epoch 113/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - loss: 0.2226 - val_loss: 0.1755\n",
      "Epoch 114/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2173 - val_loss: 0.1749\n",
      "Epoch 115/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2181 - val_loss: 0.1834\n",
      "Epoch 116/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2216 - val_loss: 0.1744\n",
      "Epoch 117/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2164 - val_loss: 0.1772\n",
      "Epoch 118/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2153 - val_loss: 0.1754\n",
      "Epoch 119/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2224 - val_loss: 0.1797\n",
      "Epoch 120/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2170 - val_loss: 0.1749\n",
      "Epoch 121/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2157 - val_loss: 0.1728\n",
      "Epoch 122/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2173 - val_loss: 0.1727\n",
      "Epoch 123/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2129 - val_loss: 0.1762\n",
      "Epoch 124/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2215 - val_loss: 0.1780\n",
      "Epoch 125/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2150 - val_loss: 0.1735\n",
      "Epoch 126/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2164 - val_loss: 0.1712\n",
      "Epoch 127/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2137 - val_loss: 0.1707\n",
      "Epoch 128/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2111 - val_loss: 0.1709\n",
      "Epoch 129/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2076 - val_loss: 0.1711\n",
      "Epoch 130/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1000us/step - loss: 0.2121 - val_loss: 0.1710\n",
      "Epoch 131/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2123 - val_loss: 0.1745\n",
      "Epoch 132/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2130 - val_loss: 0.1714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped fitting after 132 epochs\n",
      "Saved model to disk in /var/folders/t1/fw_bh5nj2sg1zsn8ft2cld2c0000gn/T/tmp_zbrc5la\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepimpute.multinet.MultiNet at 0x29fa65210>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using all the data\n",
    "multinet.fit(data,cell_subset=1,minVMR=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset is 500 cells (rows) and 3000 genes (columns)\n",
      "First 3 rows and columns:\n",
      "                  ENSG00000177954  ENSG00000197756  ENSG00000231500\n",
      "AATTGTGACTACGA-1            826.0            674.0            694.0\n",
      "TGACACGATTCGTT-1            617.0            618.0            594.0\n",
      "TGTCAGGATTGTCT-1            525.0            550.0            540.0\n",
      "3072 genes selected for imputation\n",
      "Net 0: 847 predictors, 512 targets\n",
      "Net 1: 839 predictors, 512 targets\n",
      "Net 2: 842 predictors, 512 targets\n",
      "Net 3: 840 predictors, 512 targets\n",
      "Net 4: 820 predictors, 512 targets\n",
      "Net 5: 766 predictors, 512 targets\n",
      "Normalization\n",
      "Building network\n",
      "[{'type': 'dense', 'activation': 'relu', 'neurons': 200}, {'type': 'dropout', 'activation': 'dropout', 'rate': 0.3}]\n",
      "Fitting with 250 cells\n",
      "Epoch 1/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 30.1276 - val_loss: 25.0546\n",
      "Epoch 2/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 26.7142 - val_loss: 21.8920\n",
      "Epoch 3/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 23.7704 - val_loss: 19.1379\n",
      "Epoch 4/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 21.4550 - val_loss: 16.9067\n",
      "Epoch 5/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 19.5930 - val_loss: 15.1485\n",
      "Epoch 6/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 18.1453 - val_loss: 13.7088\n",
      "Epoch 7/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 16.8479 - val_loss: 12.4962\n",
      "Epoch 8/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15.6968 - val_loss: 11.4763\n",
      "Epoch 9/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14.7484 - val_loss: 10.5920\n",
      "Epoch 10/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13.9087 - val_loss: 9.7780\n",
      "Epoch 11/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13.1547 - val_loss: 9.0155\n",
      "Epoch 12/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.5062 - val_loss: 8.3238\n",
      "Epoch 13/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.9190 - val_loss: 7.7024\n",
      "Epoch 14/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.3743 - val_loss: 7.1397\n",
      "Epoch 15/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.8473 - val_loss: 6.6395\n",
      "Epoch 16/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 10.4332 - val_loss: 6.2066\n",
      "Epoch 17/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.9408 - val_loss: 5.8124\n",
      "Epoch 18/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.6325 - val_loss: 5.4140\n",
      "Epoch 19/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.1232 - val_loss: 5.0533\n",
      "Epoch 20/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.7930 - val_loss: 4.7507\n",
      "Epoch 21/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.6057 - val_loss: 4.4819\n",
      "Epoch 22/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.2160 - val_loss: 4.2474\n",
      "Epoch 23/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.9610 - val_loss: 4.0081\n",
      "Epoch 24/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.7914 - val_loss: 3.7890\n",
      "Epoch 25/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.5306 - val_loss: 3.6118\n",
      "Epoch 26/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.3696 - val_loss: 3.4296\n",
      "Epoch 27/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.0645 - val_loss: 3.2884\n",
      "Epoch 28/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.9646 - val_loss: 3.1563\n",
      "Epoch 29/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.7614 - val_loss: 3.0327\n",
      "Epoch 30/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.5669 - val_loss: 2.9462\n",
      "Epoch 31/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.4008 - val_loss: 2.8063\n",
      "Epoch 32/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.3119 - val_loss: 2.7455\n",
      "Epoch 33/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.3365 - val_loss: 2.6607\n",
      "Epoch 34/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.0435 - val_loss: 2.5560\n",
      "Epoch 35/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 5.9730 - val_loss: 2.5088\n",
      "Epoch 36/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.7927 - val_loss: 2.5111\n",
      "Epoch 37/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.7551 - val_loss: 2.3908\n",
      "Epoch 38/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.5409 - val_loss: 2.3336\n",
      "Epoch 39/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.4776 - val_loss: 2.3304\n",
      "Epoch 40/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.4708 - val_loss: 2.2410\n",
      "Epoch 41/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.3788 - val_loss: 2.1720\n",
      "Epoch 42/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.3305 - val_loss: 2.1703\n",
      "Epoch 43/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.2014 - val_loss: 2.1809\n",
      "Epoch 44/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.1917 - val_loss: 2.0941\n",
      "Epoch 45/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.0752 - val_loss: 2.0871\n",
      "Epoch 46/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.9887 - val_loss: 2.1037\n",
      "Epoch 47/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.9722 - val_loss: 2.0105\n",
      "Epoch 48/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.9073 - val_loss: 1.9884\n",
      "Epoch 49/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.8343 - val_loss: 2.0201\n",
      "Epoch 50/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.8030 - val_loss: 1.9748\n",
      "Epoch 51/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.8082 - val_loss: 1.9166\n",
      "Epoch 52/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.7126 - val_loss: 1.9161\n",
      "Epoch 53/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.6527 - val_loss: 1.9230\n",
      "Epoch 54/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.5669 - val_loss: 1.9056\n",
      "Epoch 55/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.5007 - val_loss: 1.9314\n",
      "Epoch 56/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.5989 - val_loss: 1.8151\n",
      "Epoch 57/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.4306 - val_loss: 1.8564\n",
      "Epoch 58/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.4034 - val_loss: 1.8830\n",
      "Epoch 59/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.3710 - val_loss: 1.7880\n",
      "Epoch 60/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.2798 - val_loss: 1.8477\n",
      "Epoch 61/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.2730 - val_loss: 1.8197\n",
      "Epoch 62/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.2372 - val_loss: 1.7856\n",
      "Epoch 63/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.2241 - val_loss: 1.7839\n",
      "Epoch 64/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.1566 - val_loss: 1.7824\n",
      "Epoch 65/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.1067 - val_loss: 1.7480\n",
      "Epoch 66/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.0585 - val_loss: 1.7925\n",
      "Epoch 67/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.0652 - val_loss: 1.6904\n",
      "Epoch 68/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.0674 - val_loss: 1.7059\n",
      "Epoch 69/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.9991 - val_loss: 1.7315\n",
      "Epoch 70/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.0084 - val_loss: 1.6763\n",
      "Epoch 71/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.9527 - val_loss: 1.7091\n",
      "Epoch 72/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.8817 - val_loss: 1.7307\n",
      "Epoch 73/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.8917 - val_loss: 1.6348\n",
      "Epoch 74/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.8828 - val_loss: 1.7228\n",
      "Epoch 75/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.8049 - val_loss: 1.6624\n",
      "Epoch 76/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.8137 - val_loss: 1.6022\n",
      "Epoch 77/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.7596 - val_loss: 1.7817\n",
      "Epoch 78/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.6965 - val_loss: 1.6005\n",
      "Epoch 79/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.7512 - val_loss: 1.6053\n",
      "Epoch 80/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.6904 - val_loss: 1.6601\n",
      "Epoch 81/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.7093 - val_loss: 1.6194\n",
      "Epoch 82/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.6430 - val_loss: 1.6510\n",
      "Epoch 83/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.5674 - val_loss: 1.6474\n",
      "Stopped fitting after 83 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk in /var/folders/t1/fw_bh5nj2sg1zsn8ft2cld2c0000gn/T/tmpt3c3oytw\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepimpute.multinet.MultiNet at 0x104001a90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using 80% of the data\n",
    "multinet.fit(data,cell_subset=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset is 500 cells (rows) and 3000 genes (columns)\n",
      "First 3 rows and columns:\n",
      "                  ENSG00000177954  ENSG00000197756  ENSG00000231500\n",
      "AATTGTGACTACGA-1            826.0            674.0            694.0\n",
      "TGACACGATTCGTT-1            617.0            618.0            594.0\n",
      "TGTCAGGATTGTCT-1            525.0            550.0            540.0\n",
      "3072 genes selected for imputation\n",
      "Net 0: 907 predictors, 512 targets\n",
      "Net 1: 912 predictors, 512 targets\n",
      "Net 2: 905 predictors, 512 targets\n",
      "Net 3: 923 predictors, 512 targets\n",
      "Net 4: 898 predictors, 512 targets\n",
      "Net 5: 848 predictors, 512 targets\n",
      "Normalization\n",
      "Building network\n",
      "[{'type': 'dense', 'activation': 'relu', 'neurons': 200}, {'type': 'dropout', 'activation': 'dropout', 'rate': 0.3}]\n",
      "Fitting with 200 cells\n",
      "Epoch 1/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 29.5517 - val_loss: 24.5187\n",
      "Epoch 2/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 26.7996 - val_loss: 22.2365\n",
      "Epoch 3/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 24.4739 - val_loss: 20.0696\n",
      "Epoch 4/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 22.4037 - val_loss: 18.0961\n",
      "Epoch 5/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 20.6651 - val_loss: 16.3833\n",
      "Epoch 6/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 19.2116 - val_loss: 14.9243\n",
      "Epoch 7/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 17.9828 - val_loss: 13.7080\n",
      "Epoch 8/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 16.9935 - val_loss: 12.6996\n",
      "Epoch 9/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 16.2586 - val_loss: 11.8674\n",
      "Epoch 10/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 15.4105 - val_loss: 11.1660\n",
      "Epoch 11/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 14.8157 - val_loss: 10.5510\n",
      "Epoch 12/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 14.1864 - val_loss: 9.9892\n",
      "Epoch 13/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 13.7486 - val_loss: 9.4496\n",
      "Epoch 14/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 13.2262 - val_loss: 8.9361\n",
      "Epoch 15/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 12.6912 - val_loss: 8.4380\n",
      "Epoch 16/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 12.2952 - val_loss: 7.9641\n",
      "Epoch 17/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 11.8895 - val_loss: 7.5182\n",
      "Epoch 18/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 11.4882 - val_loss: 7.1222\n",
      "Epoch 19/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 11.1410 - val_loss: 6.7723\n",
      "Epoch 20/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 10.8602 - val_loss: 6.4389\n",
      "Epoch 21/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 10.5674 - val_loss: 6.1150\n",
      "Epoch 22/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 10.1189 - val_loss: 5.8228\n",
      "Epoch 23/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.8602 - val_loss: 5.5564\n",
      "Epoch 24/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.5630 - val_loss: 5.2999\n",
      "Epoch 25/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.3352 - val_loss: 5.0676\n",
      "Epoch 26/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.0263 - val_loss: 4.8580\n",
      "Epoch 27/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.9137 - val_loss: 4.6425\n",
      "Epoch 28/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.6275 - val_loss: 4.4509\n",
      "Epoch 29/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.5213 - val_loss: 4.2776\n",
      "Epoch 30/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.2579 - val_loss: 4.1103\n",
      "Epoch 31/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.0632 - val_loss: 3.9548\n",
      "Epoch 32/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.8880 - val_loss: 3.8185\n",
      "Epoch 33/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 7.7180 - val_loss: 3.6965\n",
      "Epoch 34/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.5705 - val_loss: 3.5833\n",
      "Epoch 35/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.4940 - val_loss: 3.4610\n",
      "Epoch 36/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.3238 - val_loss: 3.3485\n",
      "Epoch 37/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.2277 - val_loss: 3.2484\n",
      "Epoch 38/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.9901 - val_loss: 3.1518\n",
      "Epoch 39/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 6.9404 - val_loss: 3.0988\n",
      "Epoch 40/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 6.9038 - val_loss: 3.0099\n",
      "Epoch 41/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 6.6300 - val_loss: 2.9060\n",
      "Epoch 42/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 6.6101 - val_loss: 2.8338\n",
      "Epoch 43/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 6.4857 - val_loss: 2.7724\n",
      "Epoch 44/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 6.3329 - val_loss: 2.6961\n",
      "Epoch 45/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 6.2490 - val_loss: 2.6226\n",
      "Epoch 46/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.1211 - val_loss: 2.6030\n",
      "Epoch 47/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 6.0423 - val_loss: 2.5935\n",
      "Epoch 48/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.9685 - val_loss: 2.5267\n",
      "Epoch 49/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 5.9189 - val_loss: 2.4493\n",
      "Epoch 50/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.8355 - val_loss: 2.3894\n",
      "Epoch 51/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 5.7180 - val_loss: 2.3781\n",
      "Epoch 52/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 5.7145 - val_loss: 2.3280\n",
      "Epoch 53/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 5.5748 - val_loss: 2.2944\n",
      "Epoch 54/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.6430 - val_loss: 2.2279\n",
      "Epoch 55/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.4963 - val_loss: 2.2100\n",
      "Epoch 56/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.4529 - val_loss: 2.2171\n",
      "Epoch 57/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.3469 - val_loss: 2.2016\n",
      "Epoch 58/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.4571 - val_loss: 2.1222\n",
      "Epoch 59/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.3151 - val_loss: 2.0515\n",
      "Epoch 60/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.2654 - val_loss: 2.0663\n",
      "Epoch 61/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.1542 - val_loss: 2.0808\n",
      "Epoch 62/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.1158 - val_loss: 2.0399\n",
      "Epoch 63/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.0647 - val_loss: 1.9795\n",
      "Epoch 64/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.0128 - val_loss: 1.9989\n",
      "Epoch 65/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.0056 - val_loss: 1.9992\n",
      "Epoch 66/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.9101 - val_loss: 1.9582\n",
      "Epoch 67/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.9189 - val_loss: 1.9281\n",
      "Epoch 68/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.8337 - val_loss: 1.9108\n",
      "Epoch 69/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.8298 - val_loss: 1.8848\n",
      "Epoch 70/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.7522 - val_loss: 1.9009\n",
      "Epoch 71/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.7402 - val_loss: 1.8821\n",
      "Epoch 72/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.6136 - val_loss: 1.9000\n",
      "Epoch 73/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.6399 - val_loss: 1.8652\n",
      "Epoch 74/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.5814 - val_loss: 1.8476\n",
      "Epoch 75/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.6407 - val_loss: 1.7903\n",
      "Epoch 76/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.4963 - val_loss: 1.8099\n",
      "Epoch 77/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.5288 - val_loss: 1.8350\n",
      "Epoch 78/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.5027 - val_loss: 1.7710\n",
      "Epoch 79/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.4454 - val_loss: 1.7327\n",
      "Epoch 80/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.4223 - val_loss: 1.7850\n",
      "Epoch 81/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.4005 - val_loss: 1.7879\n",
      "Epoch 82/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.3435 - val_loss: 1.7326\n",
      "Epoch 83/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.3256 - val_loss: 1.7248\n",
      "Epoch 84/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.2655 - val_loss: 1.7339\n",
      "Epoch 85/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.2834 - val_loss: 1.7027\n",
      "Epoch 86/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.2618 - val_loss: 1.6804\n",
      "Epoch 87/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.1743 - val_loss: 1.7151\n",
      "Epoch 88/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.1990 - val_loss: 1.7410\n",
      "Epoch 89/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.1686 - val_loss: 1.6660\n",
      "Epoch 90/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.1385 - val_loss: 1.6422\n",
      "Epoch 91/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.1292 - val_loss: 1.7127\n",
      "Epoch 92/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.0815 - val_loss: 1.6685\n",
      "Epoch 93/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.0866 - val_loss: 1.6132\n",
      "Epoch 94/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.0500 - val_loss: 1.6400\n",
      "Epoch 95/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.9560 - val_loss: 1.6932\n",
      "Epoch 96/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.9899 - val_loss: 1.6377\n",
      "Epoch 97/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.9708 - val_loss: 1.5577\n",
      "Epoch 98/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.9672 - val_loss: 1.6138\n",
      "Epoch 99/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.8666 - val_loss: 1.7248\n",
      "Epoch 100/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.9477 - val_loss: 1.6138\n",
      "Epoch 101/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.8827 - val_loss: 1.5062\n",
      "Epoch 102/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.8258 - val_loss: 1.5936\n",
      "Epoch 103/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.8140 - val_loss: 1.7165\n",
      "Epoch 104/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.7915 - val_loss: 1.5491\n",
      "Epoch 105/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.8210 - val_loss: 1.4884\n",
      "Epoch 106/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.7799 - val_loss: 1.6032\n",
      "Epoch 107/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.7928 - val_loss: 1.5939\n",
      "Epoch 108/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.7376 - val_loss: 1.5548\n",
      "Epoch 109/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.6978 - val_loss: 1.5815\n",
      "Epoch 110/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.6517 - val_loss: 1.5664\n",
      "Stopped fitting after 110 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk in /var/folders/t1/fw_bh5nj2sg1zsn8ft2cld2c0000gn/T/tmpt3c3oytw\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepimpute.multinet.MultiNet at 0x104001a90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using 200 cells (randomly selected)\n",
    "multinet.fit(data,cell_subset=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset is 150 cells (rows) and 3000 genes (columns)\n",
      "First 3 rows and columns:\n",
      "                  ENSG00000177954  ENSG00000197756  ENSG00000231500\n",
      "AATACCCTGGGACA-1            271.0            262.0            231.0\n",
      "GGCGCATGCCTAAG-1            173.0            390.0            358.0\n",
      "CGCACTTGAACCAC-1            367.0            406.0            354.0\n",
      "3072 genes selected for imputation\n",
      "Net 0: 1252 predictors, 512 targets\n",
      "Net 1: 1263 predictors, 512 targets\n",
      "Net 2: 1250 predictors, 512 targets\n",
      "Net 3: 1273 predictors, 512 targets\n",
      "Net 4: 1253 predictors, 512 targets\n",
      "Net 5: 1251 predictors, 512 targets\n",
      "Normalization\n",
      "Building network\n",
      "[{'type': 'dense', 'activation': 'relu', 'neurons': 200}, {'type': 'dropout', 'activation': 'dropout', 'rate': 0.3}]\n",
      "Fitting with 150 cells\n",
      "Epoch 1/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 29.6420 - val_loss: 26.1024\n",
      "Epoch 2/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 26.4871 - val_loss: 23.0008\n",
      "Epoch 3/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23.7962 - val_loss: 20.2696\n",
      "Epoch 4/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 21.5475 - val_loss: 18.0390\n",
      "Epoch 5/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 19.8768 - val_loss: 16.2757\n",
      "Epoch 6/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 18.5481 - val_loss: 14.8663\n",
      "Epoch 7/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 17.4442 - val_loss: 13.7240\n",
      "Epoch 8/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 16.5234 - val_loss: 12.7893\n",
      "Epoch 9/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 15.8236 - val_loss: 12.0200\n",
      "Epoch 10/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 15.0673 - val_loss: 11.3570\n",
      "Epoch 11/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 14.4432 - val_loss: 10.7661\n",
      "Epoch 12/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 13.9057 - val_loss: 10.2117\n",
      "Epoch 13/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 13.4993 - val_loss: 9.6788\n",
      "Epoch 14/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 13.0712 - val_loss: 9.1680\n",
      "Epoch 15/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 12.5924 - val_loss: 8.6860\n",
      "Epoch 16/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 12.1326 - val_loss: 8.2300\n",
      "Epoch 17/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 11.8374 - val_loss: 7.8040\n",
      "Epoch 18/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 11.3998 - val_loss: 7.4200\n",
      "Epoch 19/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 11.0642 - val_loss: 7.0488\n",
      "Epoch 20/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 10.8423 - val_loss: 6.7087\n",
      "Epoch 21/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 10.5062 - val_loss: 6.3795\n",
      "Epoch 22/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 10.1983 - val_loss: 6.0763\n",
      "Epoch 23/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.8823 - val_loss: 5.8084\n",
      "Epoch 24/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.5823 - val_loss: 5.5782\n",
      "Epoch 25/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.4004 - val_loss: 5.3683\n",
      "Epoch 26/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.1286 - val_loss: 5.1579\n",
      "Epoch 27/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.9339 - val_loss: 4.9423\n",
      "Epoch 28/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.6474 - val_loss: 4.7337\n",
      "Epoch 29/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.5474 - val_loss: 4.5411\n",
      "Epoch 30/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.2252 - val_loss: 4.3481\n",
      "Epoch 31/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.1767 - val_loss: 4.1860\n",
      "Epoch 32/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.9214 - val_loss: 4.0290\n",
      "Epoch 33/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.7494 - val_loss: 3.8944\n",
      "Epoch 34/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.5641 - val_loss: 3.7478\n",
      "Epoch 35/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.5026 - val_loss: 3.6421\n",
      "Epoch 36/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.2740 - val_loss: 3.5634\n",
      "Epoch 37/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.3129 - val_loss: 3.4587\n",
      "Epoch 38/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.0416 - val_loss: 3.3512\n",
      "Epoch 39/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.9849 - val_loss: 3.2347\n",
      "Epoch 40/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.8547 - val_loss: 3.1287\n",
      "Epoch 41/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.6058 - val_loss: 3.1197\n",
      "Epoch 42/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6.6298 - val_loss: 3.0489\n",
      "Epoch 43/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.4270 - val_loss: 2.9295\n",
      "Epoch 44/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.3740 - val_loss: 2.8524\n",
      "Epoch 45/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6.3109 - val_loss: 2.7878\n",
      "Epoch 46/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.2012 - val_loss: 2.7304\n",
      "Epoch 47/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.1587 - val_loss: 2.7012\n",
      "Epoch 48/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.9904 - val_loss: 2.6828\n",
      "Epoch 49/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.8882 - val_loss: 2.6577\n",
      "Epoch 50/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.8060 - val_loss: 2.5695\n",
      "Epoch 51/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.7216 - val_loss: 2.5345\n",
      "Epoch 52/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.7609 - val_loss: 2.4949\n",
      "Epoch 53/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.6385 - val_loss: 2.4555\n",
      "Epoch 54/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.5936 - val_loss: 2.3846\n",
      "Epoch 55/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.5510 - val_loss: 2.3364\n",
      "Epoch 56/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.5364 - val_loss: 2.3245\n",
      "Epoch 57/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.3475 - val_loss: 2.3167\n",
      "Epoch 58/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.4740 - val_loss: 2.2618\n",
      "Epoch 59/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.2870 - val_loss: 2.2482\n",
      "Epoch 60/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.2779 - val_loss: 2.2341\n",
      "Epoch 61/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.2485 - val_loss: 2.1927\n",
      "Epoch 62/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.1651 - val_loss: 2.1462\n",
      "Epoch 63/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.1129 - val_loss: 2.1843\n",
      "Epoch 64/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.0260 - val_loss: 2.1459\n",
      "Epoch 65/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.0367 - val_loss: 2.0694\n",
      "Epoch 66/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.9643 - val_loss: 2.0752\n",
      "Epoch 67/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9478 - val_loss: 2.0926\n",
      "Epoch 68/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.9095 - val_loss: 2.0549\n",
      "Epoch 69/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.8655 - val_loss: 2.0129\n",
      "Epoch 70/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.7748 - val_loss: 2.0903\n",
      "Epoch 71/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.7967 - val_loss: 2.0566\n",
      "Epoch 72/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.7176 - val_loss: 1.9957\n",
      "Epoch 73/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.7222 - val_loss: 1.9875\n",
      "Epoch 74/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.6613 - val_loss: 2.0176\n",
      "Epoch 75/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.7274 - val_loss: 1.9143\n",
      "Epoch 76/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.5360 - val_loss: 1.9223\n",
      "Epoch 77/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.5717 - val_loss: 1.9781\n",
      "Epoch 78/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6022 - val_loss: 1.9141\n",
      "Epoch 79/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5293 - val_loss: 1.8742\n",
      "Epoch 80/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.4242 - val_loss: 1.9770\n",
      "Epoch 81/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.4198 - val_loss: 1.9844\n",
      "Epoch 82/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 4.4185 - val_loss: 1.8579\n",
      "Epoch 83/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.3414 - val_loss: 1.8354\n",
      "Epoch 84/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.3766 - val_loss: 1.8900\n",
      "Epoch 85/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.3440 - val_loss: 1.8902\n",
      "Epoch 86/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.3223 - val_loss: 1.8326\n",
      "Epoch 87/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.2599 - val_loss: 1.8571\n",
      "Epoch 88/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.2222 - val_loss: 1.8335\n",
      "Epoch 89/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.2037 - val_loss: 1.8288\n",
      "Epoch 90/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.1539 - val_loss: 1.8441\n",
      "Epoch 91/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.2088 - val_loss: 1.7827\n",
      "Epoch 92/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.1609 - val_loss: 1.7586\n",
      "Epoch 93/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.1128 - val_loss: 1.8320\n",
      "Epoch 94/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.1576 - val_loss: 1.8213\n",
      "Epoch 95/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.0132 - val_loss: 1.7439\n",
      "Epoch 96/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.0100 - val_loss: 1.7751\n",
      "Epoch 97/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.9618 - val_loss: 1.8559\n",
      "Epoch 98/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.0082 - val_loss: 1.7459\n",
      "Epoch 99/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.9401 - val_loss: 1.7430\n",
      "Epoch 100/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.0409 - val_loss: 1.7562\n",
      "Epoch 101/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.9485 - val_loss: 1.7098\n",
      "Epoch 102/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.8384 - val_loss: 1.7247\n",
      "Epoch 103/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.8137 - val_loss: 1.8583\n",
      "Epoch 104/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.8858 - val_loss: 1.7341\n",
      "Epoch 105/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.8819 - val_loss: 1.6045\n",
      "Epoch 106/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.8628 - val_loss: 1.7953\n",
      "Epoch 107/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.8452 - val_loss: 1.8263\n",
      "Epoch 108/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.8112 - val_loss: 1.6520\n",
      "Epoch 109/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.7681 - val_loss: 1.6561\n",
      "Epoch 110/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.7214 - val_loss: 1.7920\n",
      "Stopped fitting after 110 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk in /var/folders/t1/fw_bh5nj2sg1zsn8ft2cld2c0000gn/T/tmpy1yo7oy8\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepimpute.multinet.MultiNet at 0x296732710>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom fit\n",
    "trainingData = data.iloc[100:250,:]\n",
    "multinet.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "\n",
    "The imputation can be done on any dataset as long as the gene labels are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 429us/step\n",
      "Filling zeros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juank/Desktop/BCOM/Proyecto/deepimpute/deepimpute/multinet.py:287: FutureWarning: DataFrame.groupby with axis=1 is deprecated. Do `frame.T.groupby(...)` without axis instead.\n",
      "  predicted = predicted.groupby(by=predicted.columns, axis=1).mean()\n"
     ]
    }
   ],
   "source": [
    "imputedData = multinet.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1772071015_C02</th>\n",
       "      <th>1772071017_G12</th>\n",
       "      <th>1772071017_A05</th>\n",
       "      <th>1772071014_B06</th>\n",
       "      <th>1772067065_H06</th>\n",
       "      <th>1772071017_E02</th>\n",
       "      <th>1772067065_B07</th>\n",
       "      <th>1772067060_B09</th>\n",
       "      <th>1772071014_E04</th>\n",
       "      <th>1772071015_D04</th>\n",
       "      <th>...</th>\n",
       "      <th>1772067076_E04</th>\n",
       "      <th>1772066097_C06</th>\n",
       "      <th>1772067057_D12</th>\n",
       "      <th>1772066100_A05</th>\n",
       "      <th>1772071015_A11</th>\n",
       "      <th>1772071015_C09</th>\n",
       "      <th>1772062128_F10</th>\n",
       "      <th>1772071017_H06</th>\n",
       "      <th>1772071014_A12</th>\n",
       "      <th>1772063077_H05</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Atp1b2</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>5.848970</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.812623</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sub1</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>57.0</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ptprf</th>\n",
       "      <td>2.459806</td>\n",
       "      <td>2.572126</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>3.430110</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.047349</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.978819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.620672</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.210549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cers5</th>\n",
       "      <td>2.331866</td>\n",
       "      <td>2.110072</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.596802</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.425081</td>\n",
       "      <td>2.301092</td>\n",
       "      <td>1.966174</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.428072</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.749489</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rit2</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.237396</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eif1ax</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.932763</td>\n",
       "      <td>1.650007</td>\n",
       "      <td>2.051389</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.397483</td>\n",
       "      <td>1.593345</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rbbp7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.593816</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.761967</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.115924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trappc2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.48901</td>\n",
       "      <td>1.441501</td>\n",
       "      <td>1.614896</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.245944</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.220298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rab9</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.864262</td>\n",
       "      <td>1.79712</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.614426</td>\n",
       "      <td>1.888468</td>\n",
       "      <td>1.621606</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.863343</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vamp7</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.441762</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.431541</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.459471</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.290122</td>\n",
       "      <td>2.677572</td>\n",
       "      <td>1.926411</td>\n",
       "      <td>1.964262</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3529 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         1772071015_C02  1772071017_G12  1772071017_A05  1772071014_B06  \\\n",
       "Atp1b2         9.000000        5.000000             8.0        6.000000   \n",
       "Sub1          28.000000       41.000000            57.0       33.000000   \n",
       "Ptprf          2.459806        2.572126             4.0        1.000000   \n",
       "Cers5          2.331866        2.110072             7.0        1.000000   \n",
       "Rit2           2.000000        6.000000             3.0        1.000000   \n",
       "...                 ...             ...             ...             ...   \n",
       "Eif1ax         1.000000        1.000000             2.0        1.000000   \n",
       "Rbbp7          1.000000        2.000000             4.0        4.000000   \n",
       "Trappc2        1.000000        1.000000             2.0        1.000000   \n",
       "Rab9           7.000000        1.000000             1.0        3.000000   \n",
       "Vamp7          5.000000        2.441762             3.0        2.431541   \n",
       "\n",
       "         1772067065_H06  1772071017_E02  1772067065_B07  1772067060_B09  \\\n",
       "Atp1b2         7.000000         9.00000        5.848970        3.000000   \n",
       "Sub1          30.000000        13.00000       27.000000       17.000000   \n",
       "Ptprf          5.000000         2.00000        3.430110        4.000000   \n",
       "Cers5          2.596802         1.00000        2.000000        2.425081   \n",
       "Rit2          12.000000         2.00000        2.000000        2.000000   \n",
       "...                 ...             ...             ...             ...   \n",
       "Eif1ax         1.000000         1.00000        1.932763        1.650007   \n",
       "Rbbp7          6.000000         4.00000        6.000000        8.000000   \n",
       "Trappc2        2.000000         1.48901        1.441501        1.614896   \n",
       "Rab9           1.864262         1.79712        2.000000        1.614426   \n",
       "Vamp7          3.000000         2.00000        8.000000        2.000000   \n",
       "\n",
       "         1772071014_E04  1772071015_D04  ...  1772067076_E04  1772066097_C06  \\\n",
       "Atp1b2         6.000000        4.000000  ...        2.000000        1.812623   \n",
       "Sub1          23.000000       31.000000  ...       49.000000       26.000000   \n",
       "Ptprf          3.047349       10.000000  ...        1.000000        1.000000   \n",
       "Cers5          2.301092        1.966174  ...        3.000000        2.428072   \n",
       "Rit2          10.000000        7.000000  ...        2.000000        9.000000   \n",
       "...                 ...             ...  ...             ...             ...   \n",
       "Eif1ax         2.051389        1.000000  ...        2.000000        5.000000   \n",
       "Rbbp7          3.000000        4.000000  ...        1.000000        4.000000   \n",
       "Trappc2        1.000000        1.000000  ...        5.000000        6.000000   \n",
       "Rab9           1.888468        1.621606  ...        1.000000        2.000000   \n",
       "Vamp7          3.000000        3.000000  ...        3.459471        3.000000   \n",
       "\n",
       "         1772067057_D12  1772066100_A05  1772071015_A11  1772071015_C09  \\\n",
       "Atp1b2         2.000000        1.000000        2.000000        7.000000   \n",
       "Sub1          47.000000       16.000000       18.000000       27.000000   \n",
       "Ptprf          3.000000        1.978819        1.000000        7.000000   \n",
       "Cers5         13.000000        2.000000        1.749489        2.000000   \n",
       "Rit2           2.000000        2.237396        6.000000        3.000000   \n",
       "...                 ...             ...             ...             ...   \n",
       "Eif1ax         4.000000        2.000000        1.397483        1.593345   \n",
       "Rbbp7          4.000000        1.593816        2.000000        2.000000   \n",
       "Trappc2        5.000000        2.000000        1.000000        2.000000   \n",
       "Rab9           3.000000        2.000000        0.863343        6.000000   \n",
       "Vamp7          3.290122        2.677572        1.926411        1.964262   \n",
       "\n",
       "         1772062128_F10  1772071017_H06  1772071014_A12  1772063077_H05  \n",
       "Atp1b2         1.000000       10.000000             3.0        1.000000  \n",
       "Sub1          44.000000        5.000000            23.0       18.000000  \n",
       "Ptprf          7.000000        2.620672             2.0        2.210549  \n",
       "Cers5          3.000000        3.000000             4.0        1.000000  \n",
       "Rit2           5.000000       11.000000             2.0        5.000000  \n",
       "...                 ...             ...             ...             ...  \n",
       "Eif1ax         1.000000        1.000000             3.0        3.000000  \n",
       "Rbbp7          1.761967        2.000000             1.0        1.115924  \n",
       "Trappc2        1.000000        1.245944             4.0        3.220298  \n",
       "Rab9           2.000000        2.000000             2.0        3.000000  \n",
       "Vamp7          2.000000        1.000000             2.0        1.000000  \n",
       "\n",
       "[3529 rows x 200 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "limits = [0,100]\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "jitter = np.random.normal(0,1,data.size) # Add some jittering to better see the point density\n",
    "ax.scatter(data.values.flatten()+jitter,imputedData.values.flatten(),s=2)\n",
    "ax.plot(limits,limits,'r-.',linewidth=2)\n",
    "ax.set_xlim(limits)\n",
    "ax.set_ylim(limits)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "Display training metrics (MSE and Pearson's correlation on the test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'correlation': 0.8493194898683453, 'MSE': 0.16082680781214634}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinet.test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
