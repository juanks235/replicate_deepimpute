{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-cell RNA-seq imputation using DeepImpute\n",
    "\n",
    "Here is a comprehensive tutorial to understand the functionnalities of DeepImpute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 500 cells and 3000 genes\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/juank/Desktop/BCOM/Proyecto/deepimpute')\n",
    "\n",
    "from deepimpute.multinet import MultiNet\n",
    "from deepimpute.util import score_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset using pandas\n",
    "data = pd.read_csv('test.csv',index_col=0)\n",
    "print('Working on {} cells and {} genes'.format(*data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean_squared_error_metric(true_values, imputed_values):\n",
    "    return mean_squared_error(true_values, imputed_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1772071015_C02</th>\n",
       "      <th>1772071017_G12</th>\n",
       "      <th>1772071017_A05</th>\n",
       "      <th>1772071014_B06</th>\n",
       "      <th>1772067065_H06</th>\n",
       "      <th>1772071017_E02</th>\n",
       "      <th>1772067065_B07</th>\n",
       "      <th>1772067060_B09</th>\n",
       "      <th>1772071014_E04</th>\n",
       "      <th>1772071015_D04</th>\n",
       "      <th>...</th>\n",
       "      <th>1772067076_E04</th>\n",
       "      <th>1772066097_C06</th>\n",
       "      <th>1772067057_D12</th>\n",
       "      <th>1772066100_A05</th>\n",
       "      <th>1772071015_A11</th>\n",
       "      <th>1772071015_C09</th>\n",
       "      <th>1772062128_F10</th>\n",
       "      <th>1772071017_H06</th>\n",
       "      <th>1772071014_A12</th>\n",
       "      <th>1772063077_H05</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Atp1b2</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sub1</th>\n",
       "      <td>28</td>\n",
       "      <td>41</td>\n",
       "      <td>57</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>49</td>\n",
       "      <td>26</td>\n",
       "      <td>47</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>27</td>\n",
       "      <td>44</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ptprf</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cers5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rit2</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eif1ax</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rbbp7</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trappc2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rab9</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vamp7</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3529 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         1772071015_C02  1772071017_G12  1772071017_A05  1772071014_B06  \\\n",
       "Atp1b2                9               5               8               6   \n",
       "Sub1                 28              41              57              33   \n",
       "Ptprf                 0               0               4               1   \n",
       "Cers5                 0               0               7               1   \n",
       "Rit2                  2               6               3               1   \n",
       "...                 ...             ...             ...             ...   \n",
       "Eif1ax                1               1               2               1   \n",
       "Rbbp7                 1               2               4               4   \n",
       "Trappc2               1               1               2               1   \n",
       "Rab9                  7               1               1               3   \n",
       "Vamp7                 5               0               3               0   \n",
       "\n",
       "         1772067065_H06  1772071017_E02  1772067065_B07  1772067060_B09  \\\n",
       "Atp1b2                7               9               0               3   \n",
       "Sub1                 30              13              27              17   \n",
       "Ptprf                 5               2               0               4   \n",
       "Cers5                 0               1               2               0   \n",
       "Rit2                 12               2               2               2   \n",
       "...                 ...             ...             ...             ...   \n",
       "Eif1ax                1               1               0               0   \n",
       "Rbbp7                 6               4               6               8   \n",
       "Trappc2               2               0               0               0   \n",
       "Rab9                  0               0               2               0   \n",
       "Vamp7                 3               2               8               2   \n",
       "\n",
       "         1772071014_E04  1772071015_D04  ...  1772067076_E04  1772066097_C06  \\\n",
       "Atp1b2                6               4  ...               2               0   \n",
       "Sub1                 23              31  ...              49              26   \n",
       "Ptprf                 0              10  ...               1               1   \n",
       "Cers5                 0               0  ...               3               0   \n",
       "Rit2                 10               7  ...               2               9   \n",
       "...                 ...             ...  ...             ...             ...   \n",
       "Eif1ax                0               1  ...               2               5   \n",
       "Rbbp7                 3               4  ...               1               4   \n",
       "Trappc2               1               1  ...               5               6   \n",
       "Rab9                  0               0  ...               1               2   \n",
       "Vamp7                 3               3  ...               0               3   \n",
       "\n",
       "         1772067057_D12  1772066100_A05  1772071015_A11  1772071015_C09  \\\n",
       "Atp1b2                2               1               2               7   \n",
       "Sub1                 47              16              18              27   \n",
       "Ptprf                 3               0               1               7   \n",
       "Cers5                13               2               0               2   \n",
       "Rit2                  2               0               6               3   \n",
       "...                 ...             ...             ...             ...   \n",
       "Eif1ax                4               2               0               0   \n",
       "Rbbp7                 4               0               2               2   \n",
       "Trappc2               5               2               1               2   \n",
       "Rab9                  3               2               0               6   \n",
       "Vamp7                 0               0               0               0   \n",
       "\n",
       "         1772062128_F10  1772071017_H06  1772071014_A12  1772063077_H05  \n",
       "Atp1b2                1              10               3               1  \n",
       "Sub1                 44               5              23              18  \n",
       "Ptprf                 7               0               2               0  \n",
       "Cers5                 3               3               4               1  \n",
       "Rit2                  5              11               2               5  \n",
       "...                 ...             ...             ...             ...  \n",
       "Eif1ax                1               1               3               3  \n",
       "Rbbp7                 0               2               1               0  \n",
       "Trappc2               1               0               4               0  \n",
       "Rab9                  2               2               2               3  \n",
       "Vamp7                 2               1               2               1  \n",
       "\n",
       "[3529 rows x 200 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/juank/Desktop/BCOM/Proyecto/deepimpute/data/linnarsson.csv',index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000177954</th>\n",
       "      <th>ENSG00000197756</th>\n",
       "      <th>ENSG00000231500</th>\n",
       "      <th>ENSG00000140988</th>\n",
       "      <th>ENSG00000105372</th>\n",
       "      <th>ENSG00000198712</th>\n",
       "      <th>ENSG00000109475</th>\n",
       "      <th>ENSG00000112306</th>\n",
       "      <th>ENSG00000137818</th>\n",
       "      <th>ENSG00000115268</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000269858</th>\n",
       "      <th>ENSG00000182087</th>\n",
       "      <th>ENSG00000160214</th>\n",
       "      <th>ENSG00000166411</th>\n",
       "      <th>ENSG00000186153</th>\n",
       "      <th>ENSG00000089351</th>\n",
       "      <th>ENSG00000108433</th>\n",
       "      <th>ENSG00000206053</th>\n",
       "      <th>ENSG00000137806</th>\n",
       "      <th>ENSG00000197766</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AATTGTGACTACGA-1</th>\n",
       "      <td>826.0</td>\n",
       "      <td>674.0</td>\n",
       "      <td>694.0</td>\n",
       "      <td>809.0</td>\n",
       "      <td>771.0</td>\n",
       "      <td>796.0</td>\n",
       "      <td>755.0</td>\n",
       "      <td>684.0</td>\n",
       "      <td>597.0</td>\n",
       "      <td>516.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TGACACGATTCGTT-1</th>\n",
       "      <td>617.0</td>\n",
       "      <td>618.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>703.0</td>\n",
       "      <td>671.0</td>\n",
       "      <td>473.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TGTCAGGATTGTCT-1</th>\n",
       "      <td>525.0</td>\n",
       "      <td>550.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>615.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>422.0</td>\n",
       "      <td>449.0</td>\n",
       "      <td>518.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAAGTAACGTTCTT-1</th>\n",
       "      <td>514.0</td>\n",
       "      <td>474.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>447.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCCTAAACTTCATC-1</th>\n",
       "      <td>444.0</td>\n",
       "      <td>507.0</td>\n",
       "      <td>509.0</td>\n",
       "      <td>566.0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>352.0</td>\n",
       "      <td>413.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>481.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAATGGGATGCCTC-1</th>\n",
       "      <td>169.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAAAGTTGTTACCT-1</th>\n",
       "      <td>202.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGCGAACTCTAGTG-1</th>\n",
       "      <td>160.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GTCACCTGTTACCT-1</th>\n",
       "      <td>140.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TGAGTGACTACTGG-1</th>\n",
       "      <td>149.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 3000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ENSG00000177954  ENSG00000197756  ENSG00000231500  \\\n",
       "AATTGTGACTACGA-1            826.0            674.0            694.0   \n",
       "TGACACGATTCGTT-1            617.0            618.0            594.0   \n",
       "TGTCAGGATTGTCT-1            525.0            550.0            540.0   \n",
       "TAAGTAACGTTCTT-1            514.0            474.0            361.0   \n",
       "TCCTAAACTTCATC-1            444.0            507.0            509.0   \n",
       "...                           ...              ...              ...   \n",
       "AAATGGGATGCCTC-1            169.0            149.0            161.0   \n",
       "TAAAGTTGTTACCT-1            202.0            216.0            177.0   \n",
       "AGCGAACTCTAGTG-1            160.0            154.0            184.0   \n",
       "GTCACCTGTTACCT-1            140.0            133.0            179.0   \n",
       "TGAGTGACTACTGG-1            149.0            120.0            135.0   \n",
       "\n",
       "                  ENSG00000140988  ENSG00000105372  ENSG00000198712  \\\n",
       "AATTGTGACTACGA-1            809.0            771.0            796.0   \n",
       "TGACACGATTCGTT-1            703.0            671.0            473.0   \n",
       "TGTCAGGATTGTCT-1            546.0            615.0            565.0   \n",
       "TAAGTAACGTTCTT-1            331.0            447.0            279.0   \n",
       "TCCTAAACTTCATC-1            566.0            520.0            246.0   \n",
       "...                           ...              ...              ...   \n",
       "AAATGGGATGCCTC-1            203.0            178.0            155.0   \n",
       "TAAAGTTGTTACCT-1            253.0            257.0             34.0   \n",
       "AGCGAACTCTAGTG-1            253.0            150.0            190.0   \n",
       "GTCACCTGTTACCT-1            246.0            197.0             89.0   \n",
       "TGAGTGACTACTGG-1            173.0            177.0            182.0   \n",
       "\n",
       "                  ENSG00000109475  ENSG00000112306  ENSG00000137818  \\\n",
       "AATTGTGACTACGA-1            755.0            684.0            597.0   \n",
       "TGACACGATTCGTT-1            549.0            523.0            476.0   \n",
       "TGTCAGGATTGTCT-1            263.0            422.0            449.0   \n",
       "TAAGTAACGTTCTT-1            188.0            332.0            379.0   \n",
       "TCCTAAACTTCATC-1            352.0            413.0            381.0   \n",
       "...                           ...              ...              ...   \n",
       "AAATGGGATGCCTC-1            128.0            150.0            124.0   \n",
       "TAAAGTTGTTACCT-1             38.0            188.0            137.0   \n",
       "AGCGAACTCTAGTG-1            101.0            191.0            158.0   \n",
       "GTCACCTGTTACCT-1             88.0            150.0            153.0   \n",
       "TGAGTGACTACTGG-1            147.0            110.0            128.0   \n",
       "\n",
       "                  ENSG00000115268  ...  ENSG00000269858  ENSG00000182087  \\\n",
       "AATTGTGACTACGA-1            516.0  ...              3.0              2.0   \n",
       "TGACACGATTCGTT-1            527.0  ...              2.0              1.0   \n",
       "TGTCAGGATTGTCT-1            518.0  ...              2.0              1.0   \n",
       "TAAGTAACGTTCTT-1            380.0  ...              1.0              2.0   \n",
       "TCCTAAACTTCATC-1            481.0  ...              1.0              2.0   \n",
       "...                           ...  ...              ...              ...   \n",
       "AAATGGGATGCCTC-1            120.0  ...              0.0              3.0   \n",
       "TAAAGTTGTTACCT-1            189.0  ...              1.0              0.0   \n",
       "AGCGAACTCTAGTG-1            127.0  ...              0.0              0.0   \n",
       "GTCACCTGTTACCT-1            132.0  ...              2.0              1.0   \n",
       "TGAGTGACTACTGG-1            152.0  ...              1.0              0.0   \n",
       "\n",
       "                  ENSG00000160214  ENSG00000166411  ENSG00000186153  \\\n",
       "AATTGTGACTACGA-1              3.0              2.0              2.0   \n",
       "TGACACGATTCGTT-1              2.0              2.0              3.0   \n",
       "TGTCAGGATTGTCT-1              0.0              3.0              1.0   \n",
       "TAAGTAACGTTCTT-1              0.0              3.0              3.0   \n",
       "TCCTAAACTTCATC-1              0.0              5.0              4.0   \n",
       "...                           ...              ...              ...   \n",
       "AAATGGGATGCCTC-1              1.0              1.0              0.0   \n",
       "TAAAGTTGTTACCT-1              1.0              0.0              1.0   \n",
       "AGCGAACTCTAGTG-1              0.0              1.0              0.0   \n",
       "GTCACCTGTTACCT-1              1.0              3.0              1.0   \n",
       "TGAGTGACTACTGG-1              1.0              0.0              1.0   \n",
       "\n",
       "                  ENSG00000089351  ENSG00000108433  ENSG00000206053  \\\n",
       "AATTGTGACTACGA-1              0.0              0.0              1.0   \n",
       "TGACACGATTCGTT-1              3.0              1.0              4.0   \n",
       "TGTCAGGATTGTCT-1              2.0              3.0              2.0   \n",
       "TAAGTAACGTTCTT-1              0.0              1.0              0.0   \n",
       "TCCTAAACTTCATC-1              0.0              3.0              4.0   \n",
       "...                           ...              ...              ...   \n",
       "AAATGGGATGCCTC-1              0.0              0.0              0.0   \n",
       "TAAAGTTGTTACCT-1              1.0              0.0              1.0   \n",
       "AGCGAACTCTAGTG-1              0.0              0.0              1.0   \n",
       "GTCACCTGTTACCT-1              0.0              0.0              0.0   \n",
       "TGAGTGACTACTGG-1              2.0              0.0              0.0   \n",
       "\n",
       "                  ENSG00000137806  ENSG00000197766  \n",
       "AATTGTGACTACGA-1              5.0              4.0  \n",
       "TGACACGATTCGTT-1              2.0              3.0  \n",
       "TGTCAGGATTGTCT-1              2.0              1.0  \n",
       "TAAGTAACGTTCTT-1              3.0              1.0  \n",
       "TCCTAAACTTCATC-1              2.0              1.0  \n",
       "...                           ...              ...  \n",
       "AAATGGGATGCCTC-1              0.0              1.0  \n",
       "TAAAGTTGTTACCT-1              2.0              1.0  \n",
       "AGCGAACTCTAGTG-1              2.0              2.0  \n",
       "GTCACCTGTTACCT-1              3.0              1.0  \n",
       "TGAGTGACTACTGG-1              1.0              1.0  \n",
       "\n",
       "[500 rows x 3000 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('test.csv', index_col=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>^DATABASE = GeoMiame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!Database_name = Gene Expression Omnibus (GEO)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!Database_institute = NCBI NLM NIH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>!Database_web_link = http://www.ncbi.nlm.nih.g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>!Database_email = geo@ncbi.nlm.nih.gov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>^SERIES = GSE99330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37697</th>\n",
       "      <td>!Sample_relation = BioSample: https://www.ncbi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37698</th>\n",
       "      <td>!Sample_relation = SRA: https://www.ncbi.nlm.n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37699</th>\n",
       "      <td>!Sample_supplementary_file_1 = NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37700</th>\n",
       "      <td>!Sample_series_id = GSE99330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37701</th>\n",
       "      <td>!Sample_data_row_count = 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37702 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    ^DATABASE = GeoMiame\n",
       "0         !Database_name = Gene Expression Omnibus (GEO)\n",
       "1                     !Database_institute = NCBI NLM NIH\n",
       "2      !Database_web_link = http://www.ncbi.nlm.nih.g...\n",
       "3                 !Database_email = geo@ncbi.nlm.nih.gov\n",
       "4                                     ^SERIES = GSE99330\n",
       "...                                                  ...\n",
       "37697  !Sample_relation = BioSample: https://www.ncbi...\n",
       "37698  !Sample_relation = SRA: https://www.ncbi.nlm.n...\n",
       "37699                !Sample_supplementary_file_1 = NONE\n",
       "37700                       !Sample_series_id = GSE99330\n",
       "37701                         !Sample_data_row_count = 0\n",
       "\n",
       "[37702 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fish = pd.read_csv('/Users/juank/Downloads/GSE99330_family.soft', sep=\"\\t\", comment=\"#\", low_memory=False)\n",
    "fish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DeepImpute multinet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all the cores (12)\n"
     ]
    }
   ],
   "source": [
    "# Using default parameters\n",
    "multinet = MultiNet() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using custom parameters\n",
    "NN_params = {\n",
    "        'learning_rate': 1e-4,\n",
    "        'batch_size': 64,\n",
    "        'max_epochs': 200,\n",
    "        'ncores': 5,\n",
    "        'sub_outputdim': 512,\n",
    "        'architecture': [\n",
    "            {\"type\": \"dense\", \"activation\": \"relu\", \"neurons\": 200},\n",
    "            {\"type\": \"dropout\", \"activation\": \"dropout\", \"rate\": 0.3}]\n",
    "    }\n",
    "\n",
    "multinet = MultiNet(**NN_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset is 500 cells (rows) and 3000 genes (columns)\n",
      "First 3 rows and columns:\n",
      "                  ENSG00000177954  ENSG00000197756  ENSG00000231500\n",
      "AATTGTGACTACGA-1            826.0            674.0            694.0\n",
      "TGACACGATTCGTT-1            617.0            618.0            594.0\n",
      "TGTCAGGATTGTCT-1            525.0            550.0            540.0\n",
      "3072 genes selected for imputation\n",
      "Net 0: 639 predictors, 512 targets\n",
      "Net 1: 593 predictors, 512 targets\n",
      "Net 2: 591 predictors, 512 targets\n",
      "Net 3: 594 predictors, 512 targets\n",
      "Net 4: 555 predictors, 512 targets\n",
      "Net 5: 631 predictors, 512 targets\n",
      "Normalization\n",
      "Building network\n",
      "[{'type': 'dense', 'activation': 'relu', 'neurons': 200}, {'type': 'dropout', 'activation': 'dropout', 'rate': 0.3}]\n",
      "Fitting with 500 cells\n",
      "Epoch 1/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 29.7723 - val_loss: 24.7493\n",
      "Epoch 2/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 24.1000 - val_loss: 19.4970\n",
      "Epoch 3/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 19.8717 - val_loss: 15.6341\n",
      "Epoch 4/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 16.9337 - val_loss: 12.9505\n",
      "Epoch 5/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.8541 - val_loss: 10.9485\n",
      "Epoch 6/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.2134 - val_loss: 9.3088\n",
      "Epoch 7/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 11.9043 - val_loss: 8.0113\n",
      "Epoch 8/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 10.8236 - val_loss: 6.9660\n",
      "Epoch 9/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.9657 - val_loss: 6.1011\n",
      "Epoch 10/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.1615 - val_loss: 5.4266\n",
      "Epoch 11/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.6232 - val_loss: 4.7710\n",
      "Epoch 12/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.0479 - val_loss: 4.3190\n",
      "Epoch 13/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.5415 - val_loss: 3.9487\n",
      "Epoch 14/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.1911 - val_loss: 3.5463\n",
      "Epoch 15/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.8257 - val_loss: 3.3747\n",
      "Epoch 16/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.4958 - val_loss: 3.0829\n",
      "Epoch 17/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.2913 - val_loss: 2.9441\n",
      "Epoch 18/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.9643 - val_loss: 2.8482\n",
      "Epoch 19/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.7510 - val_loss: 2.6935\n",
      "Epoch 20/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.5668 - val_loss: 2.5665\n",
      "Epoch 21/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.4323 - val_loss: 2.4825\n",
      "Epoch 22/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.2690 - val_loss: 2.4102\n",
      "Epoch 23/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.1830 - val_loss: 2.3658\n",
      "Epoch 24/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.0496 - val_loss: 2.2961\n",
      "Epoch 25/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.9133 - val_loss: 2.3026\n",
      "Epoch 26/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.8596 - val_loss: 2.1245\n",
      "Epoch 27/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6964 - val_loss: 2.2025\n",
      "Epoch 28/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.5877 - val_loss: 2.0804\n",
      "Epoch 29/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.4871 - val_loss: 2.1527\n",
      "Epoch 30/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.4228 - val_loss: 2.0671\n",
      "Epoch 31/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.3246 - val_loss: 2.0347\n",
      "Epoch 32/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.2422 - val_loss: 2.0236\n",
      "Epoch 33/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.1821 - val_loss: 2.0139\n",
      "Epoch 34/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.1173 - val_loss: 1.9463\n",
      "Epoch 35/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.1103 - val_loss: 1.9567\n",
      "Epoch 36/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.9777 - val_loss: 1.9423\n",
      "Epoch 37/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.9522 - val_loss: 1.9091\n",
      "Epoch 38/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8989 - val_loss: 1.8509\n",
      "Epoch 39/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8082 - val_loss: 1.8313\n",
      "Epoch 40/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.7918 - val_loss: 1.8493\n",
      "Epoch 41/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.7106 - val_loss: 1.9138\n",
      "Epoch 42/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.6323 - val_loss: 1.7983\n",
      "Epoch 43/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5821 - val_loss: 1.8132\n",
      "Epoch 44/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.5830 - val_loss: 1.7922\n",
      "Epoch 45/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.5661 - val_loss: 1.8420\n",
      "Epoch 46/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.5376 - val_loss: 1.7754\n",
      "Epoch 47/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.4631 - val_loss: 1.7426\n",
      "Epoch 48/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.4825 - val_loss: 1.6953\n",
      "Epoch 49/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.3651 - val_loss: 1.7105\n",
      "Epoch 50/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3201 - val_loss: 1.7325\n",
      "Epoch 51/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.3256 - val_loss: 1.7222\n",
      "Epoch 52/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3031 - val_loss: 1.6755\n",
      "Epoch 53/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.2605 - val_loss: 1.6698\n",
      "Epoch 54/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.2226 - val_loss: 1.6631\n",
      "Epoch 55/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.1642 - val_loss: 1.6484\n",
      "Epoch 56/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.1611 - val_loss: 1.6634\n",
      "Epoch 57/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.1258 - val_loss: 1.6563\n",
      "Epoch 58/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.1490 - val_loss: 1.6850\n",
      "Epoch 59/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0775 - val_loss: 1.6450\n",
      "Epoch 60/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0806 - val_loss: 1.6237\n",
      "Epoch 61/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.0206 - val_loss: 1.6434\n",
      "Epoch 62/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0104 - val_loss: 1.5816\n",
      "Epoch 63/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9654 - val_loss: 1.6063\n",
      "Epoch 64/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9592 - val_loss: 1.5917\n",
      "Epoch 65/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9339 - val_loss: 1.6077\n",
      "Epoch 66/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9259 - val_loss: 1.6268\n",
      "Epoch 67/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.8954 - val_loss: 1.5880\n",
      "Stopped fitting after 67 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk in /var/folders/t1/fw_bh5nj2sg1zsn8ft2cld2c0000gn/T/tmpmw2h4y3x\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepimpute.multinet.MultiNet at 0x2b92df610>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using all the data\n",
    "multinet.fit(data,cell_subset=1,minVMR=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset is 500 cells (rows) and 3000 genes (columns)\n",
      "First 3 rows and columns:\n",
      "                  ENSG00000177954  ENSG00000197756  ENSG00000231500\n",
      "AATTGTGACTACGA-1            826.0            674.0            694.0\n",
      "TGACACGATTCGTT-1            617.0            618.0            594.0\n",
      "TGTCAGGATTGTCT-1            525.0            550.0            540.0\n",
      "3072 genes selected for imputation\n",
      "Net 0: 847 predictors, 512 targets\n",
      "Net 1: 839 predictors, 512 targets\n",
      "Net 2: 842 predictors, 512 targets\n",
      "Net 3: 840 predictors, 512 targets\n",
      "Net 4: 820 predictors, 512 targets\n",
      "Net 5: 766 predictors, 512 targets\n",
      "Normalization\n",
      "Building network\n",
      "[{'type': 'dense', 'activation': 'relu', 'neurons': 200}, {'type': 'dropout', 'activation': 'dropout', 'rate': 0.3}]\n",
      "Fitting with 250 cells\n",
      "Epoch 1/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 30.2243 - val_loss: 25.3127\n",
      "Epoch 2/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 26.7267 - val_loss: 22.0304\n",
      "Epoch 3/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 23.6759 - val_loss: 19.1078\n",
      "Epoch 4/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 21.1751 - val_loss: 16.7021\n",
      "Epoch 5/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 19.2090 - val_loss: 14.8126\n",
      "Epoch 6/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 17.6722 - val_loss: 13.3013\n",
      "Epoch 7/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 16.4635 - val_loss: 12.0569\n",
      "Epoch 8/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15.3139 - val_loss: 11.0188\n",
      "Epoch 9/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14.3527 - val_loss: 10.1523\n",
      "Epoch 10/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13.6879 - val_loss: 9.3908\n",
      "Epoch 11/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.9641 - val_loss: 8.7119\n",
      "Epoch 12/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 12.3561 - val_loss: 8.0940\n",
      "Epoch 13/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 11.7651 - val_loss: 7.5373\n",
      "Epoch 14/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.1380 - val_loss: 7.0207\n",
      "Epoch 15/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.7349 - val_loss: 6.5241\n",
      "Epoch 16/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 10.3412 - val_loss: 6.0676\n",
      "Epoch 17/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.9077 - val_loss: 5.6976\n",
      "Epoch 18/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.6095 - val_loss: 5.3539\n",
      "Epoch 19/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.2167 - val_loss: 5.0425\n",
      "Epoch 20/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.9175 - val_loss: 4.7660\n",
      "Epoch 21/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.6155 - val_loss: 4.5071\n",
      "Epoch 22/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.3715 - val_loss: 4.2384\n",
      "Epoch 23/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0455 - val_loss: 4.0205\n",
      "Epoch 24/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9681 - val_loss: 3.8429\n",
      "Epoch 25/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.6180 - val_loss: 3.6612\n",
      "Epoch 26/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.3935 - val_loss: 3.4946\n",
      "Epoch 27/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.1544 - val_loss: 3.3950\n",
      "Epoch 28/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 7.0986 - val_loss: 3.2459\n",
      "Epoch 29/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 6.8461 - val_loss: 3.0913\n",
      "Epoch 30/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.6362 - val_loss: 3.0152\n",
      "Epoch 31/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 6.5627 - val_loss: 2.8987\n",
      "Epoch 32/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.4204 - val_loss: 2.7958\n",
      "Epoch 33/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 6.3182 - val_loss: 2.7393\n",
      "Epoch 34/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 6.1440 - val_loss: 2.6699\n",
      "Epoch 35/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.9948 - val_loss: 2.6252\n",
      "Epoch 36/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.8903 - val_loss: 2.5423\n",
      "Epoch 37/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.8433 - val_loss: 2.4179\n",
      "Epoch 38/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.5630 - val_loss: 2.4252\n",
      "Epoch 39/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.5950 - val_loss: 2.3913\n",
      "Epoch 40/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.5525 - val_loss: 2.2668\n",
      "Epoch 41/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.4701 - val_loss: 2.2197\n",
      "Epoch 42/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.4079 - val_loss: 2.2333\n",
      "Epoch 43/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.2736 - val_loss: 2.2152\n",
      "Epoch 44/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.2549 - val_loss: 2.1582\n",
      "Epoch 45/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.1680 - val_loss: 2.1365\n",
      "Epoch 46/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.0655 - val_loss: 2.0988\n",
      "Epoch 47/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.0561 - val_loss: 2.0390\n",
      "Epoch 48/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.9796 - val_loss: 2.0438\n",
      "Epoch 49/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.9314 - val_loss: 2.0425\n",
      "Epoch 50/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.8771 - val_loss: 2.0234\n",
      "Epoch 51/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.8287 - val_loss: 1.9622\n",
      "Epoch 52/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.7841 - val_loss: 1.9524\n",
      "Epoch 53/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.7145 - val_loss: 1.9635\n",
      "Epoch 54/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.6698 - val_loss: 1.8660\n",
      "Epoch 55/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.5420 - val_loss: 1.9686\n",
      "Epoch 56/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.6076 - val_loss: 1.9159\n",
      "Epoch 57/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.4958 - val_loss: 1.8332\n",
      "Epoch 58/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.4480 - val_loss: 1.9164\n",
      "Epoch 59/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.4294 - val_loss: 1.8412\n",
      "Epoch 60/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.4055 - val_loss: 1.8534\n",
      "Epoch 61/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.3725 - val_loss: 1.8401\n",
      "Epoch 62/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.2906 - val_loss: 1.7916\n",
      "Epoch 63/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.2564 - val_loss: 1.7854\n",
      "Epoch 64/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.2629 - val_loss: 1.7947\n",
      "Epoch 65/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.1474 - val_loss: 1.8017\n",
      "Epoch 66/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.1504 - val_loss: 1.8149\n",
      "Epoch 67/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.1480 - val_loss: 1.6977\n",
      "Epoch 68/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.1507 - val_loss: 1.7329\n",
      "Epoch 69/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.1218 - val_loss: 1.7321\n",
      "Epoch 70/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.0581 - val_loss: 1.7154\n",
      "Epoch 71/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.0172 - val_loss: 1.7406\n",
      "Epoch 72/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.9841 - val_loss: 1.6906\n",
      "Epoch 73/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.9659 - val_loss: 1.6623\n",
      "Epoch 74/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.9404 - val_loss: 1.6915\n",
      "Epoch 75/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.8768 - val_loss: 1.6849\n",
      "Epoch 76/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.9069 - val_loss: 1.6027\n",
      "Epoch 77/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.8881 - val_loss: 1.7157\n",
      "Epoch 78/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.7926 - val_loss: 1.6474\n",
      "Epoch 79/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.8559 - val_loss: 1.5780\n",
      "Epoch 80/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.7830 - val_loss: 1.7096\n",
      "Epoch 81/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.7821 - val_loss: 1.6186\n",
      "Epoch 82/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.7303 - val_loss: 1.6635\n",
      "Epoch 83/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.6200 - val_loss: 1.6656\n",
      "Epoch 84/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.6463 - val_loss: 1.5683\n",
      "Epoch 85/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.5676 - val_loss: 1.7119\n",
      "Epoch 86/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.5988 - val_loss: 1.5945\n",
      "Epoch 87/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.6437 - val_loss: 1.5699\n",
      "Epoch 88/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5872 - val_loss: 1.6251\n",
      "Epoch 89/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6233 - val_loss: 1.5353\n",
      "Epoch 90/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.5640 - val_loss: 1.6434\n",
      "Epoch 91/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.4932 - val_loss: 1.5577\n",
      "Epoch 92/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.4876 - val_loss: 1.5391\n",
      "Epoch 93/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.4585 - val_loss: 1.5797\n",
      "Epoch 94/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.4395 - val_loss: 1.5478\n",
      "Stopped fitting after 94 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk in /var/folders/t1/fw_bh5nj2sg1zsn8ft2cld2c0000gn/T/tmpmw2h4y3x\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepimpute.multinet.MultiNet at 0x2b92df610>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using 80% of the data\n",
    "multinet.fit(data,cell_subset=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset is 500 cells (rows) and 3000 genes (columns)\n",
      "First 3 rows and columns:\n",
      "                  ENSG00000177954  ENSG00000197756  ENSG00000231500\n",
      "AATTGTGACTACGA-1            826.0            674.0            694.0\n",
      "TGACACGATTCGTT-1            617.0            618.0            594.0\n",
      "TGTCAGGATTGTCT-1            525.0            550.0            540.0\n",
      "3072 genes selected for imputation\n",
      "Net 0: 907 predictors, 512 targets\n",
      "Net 1: 912 predictors, 512 targets\n",
      "Net 2: 905 predictors, 512 targets\n",
      "Net 3: 923 predictors, 512 targets\n",
      "Net 4: 898 predictors, 512 targets\n",
      "Net 5: 848 predictors, 512 targets\n",
      "Normalization\n",
      "Building network\n",
      "[{'type': 'dense', 'activation': 'relu', 'neurons': 200}, {'type': 'dropout', 'activation': 'dropout', 'rate': 0.3}]\n",
      "Fitting with 200 cells\n",
      "Epoch 1/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 29.5999 - val_loss: 25.0739\n",
      "Epoch 2/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 27.0662 - val_loss: 22.7408\n",
      "Epoch 3/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 24.8213 - val_loss: 20.5033\n",
      "Epoch 4/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 22.7540 - val_loss: 18.4543\n",
      "Epoch 5/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 20.9619 - val_loss: 16.6584\n",
      "Epoch 6/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 19.4998 - val_loss: 15.1524\n",
      "Epoch 7/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 18.2389 - val_loss: 13.9240\n",
      "Epoch 8/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 17.2285 - val_loss: 12.9224\n",
      "Epoch 9/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 16.3842 - val_loss: 12.1055\n",
      "Epoch 10/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 15.6842 - val_loss: 11.4333\n",
      "Epoch 11/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 14.9426 - val_loss: 10.8601\n",
      "Epoch 12/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 14.3783 - val_loss: 10.3369\n",
      "Epoch 13/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 13.8857 - val_loss: 9.8263\n",
      "Epoch 14/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 13.3802 - val_loss: 9.3162\n",
      "Epoch 15/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 12.8376 - val_loss: 8.8128\n",
      "Epoch 16/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 12.3856 - val_loss: 8.3314\n",
      "Epoch 17/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 12.0483 - val_loss: 7.8821\n",
      "Epoch 18/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 11.5620 - val_loss: 7.4794\n",
      "Epoch 19/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 11.2231 - val_loss: 7.1159\n",
      "Epoch 20/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 10.9553 - val_loss: 6.7676\n",
      "Epoch 21/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 10.5352 - val_loss: 6.4484\n",
      "Epoch 22/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 10.2268 - val_loss: 6.1585\n",
      "Epoch 23/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 9.9349 - val_loss: 5.8828\n",
      "Epoch 24/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 9.6627 - val_loss: 5.5920\n",
      "Epoch 25/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 9.3350 - val_loss: 5.3144\n",
      "Epoch 26/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 9.1235 - val_loss: 5.0770\n",
      "Epoch 27/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.8931 - val_loss: 4.8527\n",
      "Epoch 28/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.6771 - val_loss: 4.6423\n",
      "Epoch 29/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.4659 - val_loss: 4.4690\n",
      "Epoch 30/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.2674 - val_loss: 4.2974\n",
      "Epoch 31/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.1649 - val_loss: 4.1142\n",
      "Epoch 32/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.8693 - val_loss: 3.9523\n",
      "Epoch 33/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.7523 - val_loss: 3.8030\n",
      "Epoch 34/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 7.5447 - val_loss: 3.6554\n",
      "Epoch 35/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.4315 - val_loss: 3.5247\n",
      "Epoch 36/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.1732 - val_loss: 3.4315\n",
      "Epoch 37/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.1206 - val_loss: 3.3588\n",
      "Epoch 38/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.0011 - val_loss: 3.2614\n",
      "Epoch 39/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6.8294 - val_loss: 3.1583\n",
      "Epoch 40/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.8025 - val_loss: 3.0347\n",
      "Epoch 41/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 6.5665 - val_loss: 2.9202\n",
      "Epoch 42/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 6.5051 - val_loss: 2.8400\n",
      "Epoch 43/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 6.3411 - val_loss: 2.7870\n",
      "Epoch 44/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 6.2400 - val_loss: 2.7561\n",
      "Epoch 45/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 6.1440 - val_loss: 2.6970\n",
      "Epoch 46/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 6.0546 - val_loss: 2.6148\n",
      "Epoch 47/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.9590 - val_loss: 2.5775\n",
      "Epoch 48/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.8536 - val_loss: 2.5379\n",
      "Epoch 49/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.7770 - val_loss: 2.4663\n",
      "Epoch 50/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.6793 - val_loss: 2.3979\n",
      "Epoch 51/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.6049 - val_loss: 2.4061\n",
      "Epoch 52/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.6232 - val_loss: 2.3663\n",
      "Epoch 53/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.5190 - val_loss: 2.2578\n",
      "Epoch 54/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.4226 - val_loss: 2.1996\n",
      "Epoch 55/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.4083 - val_loss: 2.2010\n",
      "Epoch 56/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.3488 - val_loss: 2.1754\n",
      "Epoch 57/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.2076 - val_loss: 2.1730\n",
      "Epoch 58/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.3126 - val_loss: 2.1319\n",
      "Epoch 59/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.1793 - val_loss: 2.0602\n",
      "Epoch 60/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.1154 - val_loss: 2.0366\n",
      "Epoch 61/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.0056 - val_loss: 2.0624\n",
      "Epoch 62/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.0640 - val_loss: 2.0117\n",
      "Epoch 63/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.9510 - val_loss: 1.9654\n",
      "Epoch 64/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.9439 - val_loss: 1.9839\n",
      "Epoch 65/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.8809 - val_loss: 1.9650\n",
      "Epoch 66/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.8693 - val_loss: 1.9162\n",
      "Epoch 67/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.7884 - val_loss: 1.8915\n",
      "Epoch 68/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.7163 - val_loss: 1.9079\n",
      "Epoch 69/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.7438 - val_loss: 1.9054\n",
      "Epoch 70/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.6041 - val_loss: 1.9003\n",
      "Epoch 71/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.6567 - val_loss: 1.8336\n",
      "Epoch 72/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.6864 - val_loss: 1.8081\n",
      "Epoch 73/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.5747 - val_loss: 1.8398\n",
      "Epoch 74/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.4576 - val_loss: 1.8925\n",
      "Epoch 75/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 4.5616 - val_loss: 1.7792\n",
      "Epoch 76/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.4307 - val_loss: 1.7525\n",
      "Epoch 77/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.4378 - val_loss: 1.8294\n",
      "Epoch 78/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.3326 - val_loss: 1.8164\n",
      "Epoch 79/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.3972 - val_loss: 1.7231\n",
      "Epoch 80/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.3167 - val_loss: 1.7229\n",
      "Epoch 81/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.3200 - val_loss: 1.7683\n",
      "Epoch 82/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.2471 - val_loss: 1.7530\n",
      "Epoch 83/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.2221 - val_loss: 1.6972\n",
      "Epoch 84/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.2285 - val_loss: 1.6759\n",
      "Epoch 85/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.2335 - val_loss: 1.7065\n",
      "Epoch 86/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.2114 - val_loss: 1.6676\n",
      "Epoch 87/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.0731 - val_loss: 1.7104\n",
      "Epoch 88/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.0887 - val_loss: 1.7321\n",
      "Epoch 89/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.0630 - val_loss: 1.6525\n",
      "Epoch 90/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 4.0523 - val_loss: 1.6405\n",
      "Epoch 91/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.0628 - val_loss: 1.7076\n",
      "Epoch 92/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.0263 - val_loss: 1.6488\n",
      "Epoch 93/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.9773 - val_loss: 1.6524\n",
      "Epoch 94/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.9585 - val_loss: 1.6478\n",
      "Epoch 95/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.8676 - val_loss: 1.6490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped fitting after 95 epochs\n",
      "Saved model to disk in /var/folders/t1/fw_bh5nj2sg1zsn8ft2cld2c0000gn/T/tmpmw2h4y3x\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepimpute.multinet.MultiNet at 0x2b92df610>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using 200 cells (randomly selected)\n",
    "multinet.fit(data,cell_subset=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset is 150 cells (rows) and 3000 genes (columns)\n",
      "First 3 rows and columns:\n",
      "                  ENSG00000177954  ENSG00000197756  ENSG00000231500\n",
      "AATACCCTGGGACA-1            271.0            262.0            231.0\n",
      "GGCGCATGCCTAAG-1            173.0            390.0            358.0\n",
      "CGCACTTGAACCAC-1            367.0            406.0            354.0\n",
      "3072 genes selected for imputation\n",
      "Net 0: 1252 predictors, 512 targets\n",
      "Net 1: 1263 predictors, 512 targets\n",
      "Net 2: 1250 predictors, 512 targets\n",
      "Net 3: 1273 predictors, 512 targets\n",
      "Net 4: 1253 predictors, 512 targets\n",
      "Net 5: 1251 predictors, 512 targets\n",
      "Normalization\n",
      "Building network\n",
      "[{'type': 'dense', 'activation': 'relu', 'neurons': 200}, {'type': 'dropout', 'activation': 'dropout', 'rate': 0.3}]\n",
      "Fitting with 150 cells\n",
      "Epoch 1/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 29.5065 - val_loss: 25.9750\n",
      "Epoch 2/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 26.5709 - val_loss: 22.9748\n",
      "Epoch 3/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23.9062 - val_loss: 20.2825\n",
      "Epoch 4/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 21.5721 - val_loss: 18.0180\n",
      "Epoch 5/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 19.9881 - val_loss: 16.1633\n",
      "Epoch 6/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 18.5944 - val_loss: 14.6573\n",
      "Epoch 7/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 17.3789 - val_loss: 13.4431\n",
      "Epoch 8/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 16.4205 - val_loss: 12.4755\n",
      "Epoch 9/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 15.6280 - val_loss: 11.6914\n",
      "Epoch 10/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 14.8974 - val_loss: 11.0229\n",
      "Epoch 11/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 14.3132 - val_loss: 10.4069\n",
      "Epoch 12/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 13.7110 - val_loss: 9.8019\n",
      "Epoch 13/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 13.2080 - val_loss: 9.1985\n",
      "Epoch 14/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 12.6787 - val_loss: 8.6538\n",
      "Epoch 15/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 12.2927 - val_loss: 8.1869\n",
      "Epoch 16/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 11.7993 - val_loss: 7.7791\n",
      "Epoch 17/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 11.5059 - val_loss: 7.3945\n",
      "Epoch 18/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 11.0794 - val_loss: 7.0213\n",
      "Epoch 19/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 10.7883 - val_loss: 6.6436\n",
      "Epoch 20/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 10.4142 - val_loss: 6.2759\n",
      "Epoch 21/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 10.1248 - val_loss: 5.9612\n",
      "Epoch 22/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.8665 - val_loss: 5.6762\n",
      "Epoch 23/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.5790 - val_loss: 5.4200\n",
      "Epoch 24/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.2383 - val_loss: 5.1876\n",
      "Epoch 25/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.8944 - val_loss: 4.9987\n",
      "Epoch 26/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.7457 - val_loss: 4.8232\n",
      "Epoch 27/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.6418 - val_loss: 4.6344\n",
      "Epoch 28/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.2971 - val_loss: 4.4375\n",
      "Epoch 29/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 8.2725 - val_loss: 4.2492\n",
      "Epoch 30/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.1080 - val_loss: 4.0440\n",
      "Epoch 31/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.8516 - val_loss: 3.8972\n",
      "Epoch 32/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.6688 - val_loss: 3.7974\n",
      "Epoch 33/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.5983 - val_loss: 3.7045\n",
      "Epoch 34/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.3954 - val_loss: 3.5649\n",
      "Epoch 35/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.3531 - val_loss: 3.4479\n",
      "Epoch 36/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.1323 - val_loss: 3.3314\n",
      "Epoch 37/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.0430 - val_loss: 3.2459\n",
      "Epoch 38/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6.8018 - val_loss: 3.2426\n",
      "Epoch 39/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6.7399 - val_loss: 3.1858\n",
      "Epoch 40/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.6386 - val_loss: 3.0181\n",
      "Epoch 41/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.5247 - val_loss: 2.9087\n",
      "Epoch 42/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6.4931 - val_loss: 2.8665\n",
      "Epoch 43/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6.2785 - val_loss: 2.8251\n",
      "Epoch 44/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6.1958 - val_loss: 2.7932\n",
      "Epoch 45/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6.1999 - val_loss: 2.7040\n",
      "Epoch 46/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.0626 - val_loss: 2.6043\n",
      "Epoch 47/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6.0230 - val_loss: 2.6125\n",
      "Epoch 48/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.8836 - val_loss: 2.6323\n",
      "Epoch 49/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.8283 - val_loss: 2.5730\n",
      "Epoch 50/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.7150 - val_loss: 2.4866\n",
      "Epoch 51/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.5738 - val_loss: 2.5093\n",
      "Epoch 52/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.5728 - val_loss: 2.5120\n",
      "Epoch 53/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.5875 - val_loss: 2.4002\n",
      "Epoch 54/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 5.5154 - val_loss: 2.2679\n",
      "Epoch 55/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.4663 - val_loss: 2.2706\n",
      "Epoch 56/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.4115 - val_loss: 2.3384\n",
      "Epoch 57/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.2333 - val_loss: 2.3726\n",
      "Epoch 58/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.3260 - val_loss: 2.2749\n",
      "Epoch 59/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.2118 - val_loss: 2.1565\n",
      "Epoch 60/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.2066 - val_loss: 2.1394\n",
      "Epoch 61/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.1213 - val_loss: 2.1939\n",
      "Epoch 62/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.0913 - val_loss: 2.2542\n",
      "Epoch 63/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.9941 - val_loss: 2.1607\n",
      "Epoch 64/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.9736 - val_loss: 2.0825\n",
      "Epoch 65/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.9775 - val_loss: 2.1186\n",
      "Epoch 66/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.8530 - val_loss: 2.1806\n",
      "Epoch 67/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.9190 - val_loss: 2.0766\n",
      "Epoch 68/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.8389 - val_loss: 1.9706\n",
      "Epoch 69/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.7648 - val_loss: 2.0157\n",
      "Epoch 70/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.7199 - val_loss: 2.1761\n",
      "Epoch 71/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.7461 - val_loss: 2.0669\n",
      "Epoch 72/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.6531 - val_loss: 1.9734\n",
      "Epoch 73/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.5894 - val_loss: 2.0222\n",
      "Stopped fitting after 73 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk in /var/folders/t1/fw_bh5nj2sg1zsn8ft2cld2c0000gn/T/tmpmw2h4y3x\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepimpute.multinet.MultiNet at 0x2b92df610>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom fit\n",
    "trainingData = data.iloc[100:250,:]\n",
    "multinet.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "\n",
    "The imputation can be done on any dataset as long as the gene labels are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x2bc3c3ba0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x2bc3c3ba0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Filling zeros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juank/Desktop/BCOM/Proyecto/deepimpute/deepimpute/multinet.py:287: FutureWarning: DataFrame.groupby with axis=1 is deprecated. Do `frame.T.groupby(...)` without axis instead.\n",
      "  predicted = predicted.groupby(by=predicted.columns, axis=1).mean()\n"
     ]
    }
   ],
   "source": [
    "imputedData = multinet.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000177954</th>\n",
       "      <th>ENSG00000197756</th>\n",
       "      <th>ENSG00000231500</th>\n",
       "      <th>ENSG00000140988</th>\n",
       "      <th>ENSG00000105372</th>\n",
       "      <th>ENSG00000198712</th>\n",
       "      <th>ENSG00000109475</th>\n",
       "      <th>ENSG00000112306</th>\n",
       "      <th>ENSG00000137818</th>\n",
       "      <th>ENSG00000115268</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000269858</th>\n",
       "      <th>ENSG00000182087</th>\n",
       "      <th>ENSG00000160214</th>\n",
       "      <th>ENSG00000166411</th>\n",
       "      <th>ENSG00000186153</th>\n",
       "      <th>ENSG00000089351</th>\n",
       "      <th>ENSG00000108433</th>\n",
       "      <th>ENSG00000206053</th>\n",
       "      <th>ENSG00000137806</th>\n",
       "      <th>ENSG00000197766</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AATTGTGACTACGA-1</th>\n",
       "      <td>826.0</td>\n",
       "      <td>674.0</td>\n",
       "      <td>694.0</td>\n",
       "      <td>809.0</td>\n",
       "      <td>771.0</td>\n",
       "      <td>796.0</td>\n",
       "      <td>755.0</td>\n",
       "      <td>684.0</td>\n",
       "      <td>597.0</td>\n",
       "      <td>516.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.728606</td>\n",
       "      <td>1.487752</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TGACACGATTCGTT-1</th>\n",
       "      <td>617.0</td>\n",
       "      <td>618.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>703.0</td>\n",
       "      <td>671.0</td>\n",
       "      <td>473.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TGTCAGGATTGTCT-1</th>\n",
       "      <td>525.0</td>\n",
       "      <td>550.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>615.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>422.0</td>\n",
       "      <td>449.0</td>\n",
       "      <td>518.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.276214</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAAGTAACGTTCTT-1</th>\n",
       "      <td>514.0</td>\n",
       "      <td>474.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>447.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.664322</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.131821</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.653391</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCCTAAACTTCATC-1</th>\n",
       "      <td>444.0</td>\n",
       "      <td>507.0</td>\n",
       "      <td>509.0</td>\n",
       "      <td>566.0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>352.0</td>\n",
       "      <td>413.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>481.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.245376</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.441857</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAATGGGATGCCTC-1</th>\n",
       "      <td>169.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.194987</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.713708</td>\n",
       "      <td>1.142117</td>\n",
       "      <td>1.112967</td>\n",
       "      <td>1.009638</td>\n",
       "      <td>1.095456</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAAAGTTGTTACCT-1</th>\n",
       "      <td>202.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.281344</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.791254</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.922823</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGCGAACTCTAGTG-1</th>\n",
       "      <td>160.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.807915</td>\n",
       "      <td>1.567907</td>\n",
       "      <td>0.967640</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.581422</td>\n",
       "      <td>1.319356</td>\n",
       "      <td>1.598442</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GTCACCTGTTACCT-1</th>\n",
       "      <td>140.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950714</td>\n",
       "      <td>1.421878</td>\n",
       "      <td>1.076663</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TGAGTGACTACTGG-1</th>\n",
       "      <td>149.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.194407</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.002928</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.234182</td>\n",
       "      <td>1.129884</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 3000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ENSG00000177954  ENSG00000197756  ENSG00000231500  \\\n",
       "AATTGTGACTACGA-1            826.0            674.0            694.0   \n",
       "TGACACGATTCGTT-1            617.0            618.0            594.0   \n",
       "TGTCAGGATTGTCT-1            525.0            550.0            540.0   \n",
       "TAAGTAACGTTCTT-1            514.0            474.0            361.0   \n",
       "TCCTAAACTTCATC-1            444.0            507.0            509.0   \n",
       "...                           ...              ...              ...   \n",
       "AAATGGGATGCCTC-1            169.0            149.0            161.0   \n",
       "TAAAGTTGTTACCT-1            202.0            216.0            177.0   \n",
       "AGCGAACTCTAGTG-1            160.0            154.0            184.0   \n",
       "GTCACCTGTTACCT-1            140.0            133.0            179.0   \n",
       "TGAGTGACTACTGG-1            149.0            120.0            135.0   \n",
       "\n",
       "                  ENSG00000140988  ENSG00000105372  ENSG00000198712  \\\n",
       "AATTGTGACTACGA-1            809.0            771.0            796.0   \n",
       "TGACACGATTCGTT-1            703.0            671.0            473.0   \n",
       "TGTCAGGATTGTCT-1            546.0            615.0            565.0   \n",
       "TAAGTAACGTTCTT-1            331.0            447.0            279.0   \n",
       "TCCTAAACTTCATC-1            566.0            520.0            246.0   \n",
       "...                           ...              ...              ...   \n",
       "AAATGGGATGCCTC-1            203.0            178.0            155.0   \n",
       "TAAAGTTGTTACCT-1            253.0            257.0             34.0   \n",
       "AGCGAACTCTAGTG-1            253.0            150.0            190.0   \n",
       "GTCACCTGTTACCT-1            246.0            197.0             89.0   \n",
       "TGAGTGACTACTGG-1            173.0            177.0            182.0   \n",
       "\n",
       "                  ENSG00000109475  ENSG00000112306  ENSG00000137818  \\\n",
       "AATTGTGACTACGA-1            755.0            684.0            597.0   \n",
       "TGACACGATTCGTT-1            549.0            523.0            476.0   \n",
       "TGTCAGGATTGTCT-1            263.0            422.0            449.0   \n",
       "TAAGTAACGTTCTT-1            188.0            332.0            379.0   \n",
       "TCCTAAACTTCATC-1            352.0            413.0            381.0   \n",
       "...                           ...              ...              ...   \n",
       "AAATGGGATGCCTC-1            128.0            150.0            124.0   \n",
       "TAAAGTTGTTACCT-1             38.0            188.0            137.0   \n",
       "AGCGAACTCTAGTG-1            101.0            191.0            158.0   \n",
       "GTCACCTGTTACCT-1             88.0            150.0            153.0   \n",
       "TGAGTGACTACTGG-1            147.0            110.0            128.0   \n",
       "\n",
       "                  ENSG00000115268  ...  ENSG00000269858  ENSG00000182087  \\\n",
       "AATTGTGACTACGA-1            516.0  ...         3.000000         2.000000   \n",
       "TGACACGATTCGTT-1            527.0  ...         2.000000         1.000000   \n",
       "TGTCAGGATTGTCT-1            518.0  ...         2.000000         1.000000   \n",
       "TAAGTAACGTTCTT-1            380.0  ...         1.000000         2.000000   \n",
       "TCCTAAACTTCATC-1            481.0  ...         1.000000         2.000000   \n",
       "...                           ...  ...              ...              ...   \n",
       "AAATGGGATGCCTC-1            120.0  ...         1.194987         3.000000   \n",
       "TAAAGTTGTTACCT-1            189.0  ...         1.000000         1.281344   \n",
       "AGCGAACTCTAGTG-1            127.0  ...         1.807915         1.567907   \n",
       "GTCACCTGTTACCT-1            132.0  ...         2.000000         1.000000   \n",
       "TGAGTGACTACTGG-1            152.0  ...         1.000000         1.194407   \n",
       "\n",
       "                  ENSG00000160214  ENSG00000166411  ENSG00000186153  \\\n",
       "AATTGTGACTACGA-1         3.000000         2.000000         2.000000   \n",
       "TGACACGATTCGTT-1         2.000000         2.000000         3.000000   \n",
       "TGTCAGGATTGTCT-1         2.276214         3.000000         1.000000   \n",
       "TAAGTAACGTTCTT-1         1.664322         3.000000         3.000000   \n",
       "TCCTAAACTTCATC-1         2.245376         5.000000         4.000000   \n",
       "...                           ...              ...              ...   \n",
       "AAATGGGATGCCTC-1         1.000000         1.000000         1.713708   \n",
       "TAAAGTTGTTACCT-1         1.000000         1.791254         1.000000   \n",
       "AGCGAACTCTAGTG-1         0.967640         1.000000         0.581422   \n",
       "GTCACCTGTTACCT-1         1.000000         3.000000         1.000000   \n",
       "TGAGTGACTACTGG-1         1.000000         2.002928         1.000000   \n",
       "\n",
       "                  ENSG00000089351  ENSG00000108433  ENSG00000206053  \\\n",
       "AATTGTGACTACGA-1         1.728606         1.487752         1.000000   \n",
       "TGACACGATTCGTT-1         3.000000         1.000000         4.000000   \n",
       "TGTCAGGATTGTCT-1         2.000000         3.000000         2.000000   \n",
       "TAAGTAACGTTCTT-1         1.131821         1.000000         1.653391   \n",
       "TCCTAAACTTCATC-1         1.441857         3.000000         4.000000   \n",
       "...                           ...              ...              ...   \n",
       "AAATGGGATGCCTC-1         1.142117         1.112967         1.009638   \n",
       "TAAAGTTGTTACCT-1         1.000000         0.922823         1.000000   \n",
       "AGCGAACTCTAGTG-1         1.319356         1.598442         1.000000   \n",
       "GTCACCTGTTACCT-1         0.950714         1.421878         1.076663   \n",
       "TGAGTGACTACTGG-1         2.000000         1.234182         1.129884   \n",
       "\n",
       "                  ENSG00000137806  ENSG00000197766  \n",
       "AATTGTGACTACGA-1         5.000000              4.0  \n",
       "TGACACGATTCGTT-1         2.000000              3.0  \n",
       "TGTCAGGATTGTCT-1         2.000000              1.0  \n",
       "TAAGTAACGTTCTT-1         3.000000              1.0  \n",
       "TCCTAAACTTCATC-1         2.000000              1.0  \n",
       "...                           ...              ...  \n",
       "AAATGGGATGCCTC-1         1.095456              1.0  \n",
       "TAAAGTTGTTACCT-1         2.000000              1.0  \n",
       "AGCGAACTCTAGTG-1         2.000000              2.0  \n",
       "GTCACCTGTTACCT-1         3.000000              1.0  \n",
       "TGAGTGACTACTGG-1         1.000000              1.0  \n",
       "\n",
       "[500 rows x 3000 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "limits = [0,100]\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "jitter = np.random.normal(0,1,data.size) # Add some jittering to better see the point density\n",
    "ax.scatter(data.values.flatten()+jitter,imputedData.values.flatten(),s=2)\n",
    "ax.plot(limits,limits,'r-.',linewidth=2)\n",
    "ax.set_xlim(limits)\n",
    "ax.set_ylim(limits)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "Display training metrics (MSE and Pearson's correlation on the test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'correlation': 0.8885368453106397, 'MSE': 0.20584989830854902}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinet.test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Filling zeros\n",
      "8.912729702805057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juank/Desktop/BCOM/Proyecto/deepimpute/deepimpute/multinet.py:287: FutureWarning: DataFrame.groupby with axis=1 is deprecated. Do `frame.T.groupby(...)` without axis instead.\n",
      "  predicted = predicted.groupby(by=predicted.columns, axis=1).mean()\n"
     ]
    }
   ],
   "source": [
    "print(score_model(multinet, data, mean_squared_error_metric, cols=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neuron9k Gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 9128 cells and 27998 genes\n"
     ]
    }
   ],
   "source": [
    "# Ruta a la carpeta que contiene los archivos 'genes.tsv', 'barcodes.tsv' y 'matrix.mtx'\n",
    "data_dir = '/Users/juank/Desktop/BCOM/Proyecto/deepimpute/data/neuron9k'\n",
    "\n",
    "# Leer los datos usando scanpy\n",
    "adata = sc.read_10x_mtx(data_dir, var_names='gene_symbols', cache=True)\n",
    "\n",
    "# Convertir AnnData a DataFrame de Pandas\n",
    "# Las filas son células y las columnas son genes\n",
    "data = adata.to_df()\n",
    "print('Working on {} cells and {} genes'.format(*data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Xkr4</th>\n",
       "      <th>Gm1992</th>\n",
       "      <th>Gm37381</th>\n",
       "      <th>Rp1</th>\n",
       "      <th>Rp1-1</th>\n",
       "      <th>Sox17</th>\n",
       "      <th>Gm37323</th>\n",
       "      <th>Mrpl15</th>\n",
       "      <th>Lypla1</th>\n",
       "      <th>Gm37988</th>\n",
       "      <th>...</th>\n",
       "      <th>AC125149.1</th>\n",
       "      <th>AC125149.2</th>\n",
       "      <th>AC125149.4</th>\n",
       "      <th>AC234645.1</th>\n",
       "      <th>AC168977.2</th>\n",
       "      <th>AC168977.1</th>\n",
       "      <th>PISD</th>\n",
       "      <th>DHRSX</th>\n",
       "      <th>Vmn2r122</th>\n",
       "      <th>CAAA01147332.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAACCTGAGACGCTTT-1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCTGAGATCTGCT-1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCTGAGGTCATCT-1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCTGCAAGAGTCG-1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCTGCACACATGT-1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTCAGTCTTGATG-1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTCAGTGATAAAC-1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTCATCCTTGACC-1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTCATCGGCTACG-1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTCATCGTCCGTT-1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9128 rows × 27998 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Xkr4  Gm1992  Gm37381  Rp1  Rp1-1  Sox17  Gm37323  Mrpl15  \\\n",
       "AAACCTGAGACGCTTT-1   0.0     0.0      0.0  0.0    0.0    0.0      0.0     0.0   \n",
       "AAACCTGAGATCTGCT-1   0.0     0.0      0.0  0.0    0.0    0.0      0.0     1.0   \n",
       "AAACCTGAGGTCATCT-1   0.0     0.0      0.0  0.0    0.0    0.0      0.0     1.0   \n",
       "AAACCTGCAAGAGTCG-1   0.0     0.0      0.0  0.0    0.0    0.0      0.0     0.0   \n",
       "AAACCTGCACACATGT-1   0.0     0.0      0.0  0.0    0.0    0.0      0.0     1.0   \n",
       "...                  ...     ...      ...  ...    ...    ...      ...     ...   \n",
       "TTTGTCAGTCTTGATG-1   0.0     0.0      0.0  0.0    0.0    0.0      0.0     1.0   \n",
       "TTTGTCAGTGATAAAC-1   0.0     0.0      0.0  0.0    0.0    0.0      0.0     0.0   \n",
       "TTTGTCATCCTTGACC-1   0.0     0.0      0.0  0.0    0.0    0.0      0.0     2.0   \n",
       "TTTGTCATCGGCTACG-1   0.0     0.0      0.0  0.0    0.0    0.0      0.0     0.0   \n",
       "TTTGTCATCGTCCGTT-1   0.0     0.0      0.0  0.0    0.0    0.0      0.0     0.0   \n",
       "\n",
       "                    Lypla1  Gm37988  ...  AC125149.1  AC125149.2  AC125149.4  \\\n",
       "AAACCTGAGACGCTTT-1     2.0      0.0  ...         0.0         0.0         0.0   \n",
       "AAACCTGAGATCTGCT-1     0.0      0.0  ...         0.0         0.0         0.0   \n",
       "AAACCTGAGGTCATCT-1     0.0      0.0  ...         0.0         0.0         0.0   \n",
       "AAACCTGCAAGAGTCG-1     0.0      0.0  ...         0.0         0.0         0.0   \n",
       "AAACCTGCACACATGT-1     0.0      0.0  ...         0.0         0.0         0.0   \n",
       "...                    ...      ...  ...         ...         ...         ...   \n",
       "TTTGTCAGTCTTGATG-1     0.0      0.0  ...         0.0         0.0         0.0   \n",
       "TTTGTCAGTGATAAAC-1     0.0      0.0  ...         0.0         0.0         0.0   \n",
       "TTTGTCATCCTTGACC-1     0.0      0.0  ...         0.0         0.0         0.0   \n",
       "TTTGTCATCGGCTACG-1     0.0      0.0  ...         0.0         0.0         0.0   \n",
       "TTTGTCATCGTCCGTT-1     1.0      0.0  ...         0.0         0.0         0.0   \n",
       "\n",
       "                    AC234645.1  AC168977.2  AC168977.1  PISD  DHRSX  Vmn2r122  \\\n",
       "AAACCTGAGACGCTTT-1         0.0         0.0         0.0   2.0    0.0       0.0   \n",
       "AAACCTGAGATCTGCT-1         0.0         0.0         0.0   0.0    0.0       0.0   \n",
       "AAACCTGAGGTCATCT-1         0.0         0.0         0.0   1.0    0.0       0.0   \n",
       "AAACCTGCAAGAGTCG-1         0.0         0.0         0.0   3.0    0.0       0.0   \n",
       "AAACCTGCACACATGT-1         0.0         0.0         0.0   1.0    0.0       0.0   \n",
       "...                        ...         ...         ...   ...    ...       ...   \n",
       "TTTGTCAGTCTTGATG-1         0.0         0.0         0.0   3.0    0.0       0.0   \n",
       "TTTGTCAGTGATAAAC-1         0.0         0.0         0.0   1.0    0.0       0.0   \n",
       "TTTGTCATCCTTGACC-1         0.0         0.0         0.0   5.0    1.0       0.0   \n",
       "TTTGTCATCGGCTACG-1         0.0         0.0         0.0   0.0    0.0       0.0   \n",
       "TTTGTCATCGTCCGTT-1         0.0         0.0         0.0   2.0    1.0       0.0   \n",
       "\n",
       "                    CAAA01147332.1  \n",
       "AAACCTGAGACGCTTT-1             0.0  \n",
       "AAACCTGAGATCTGCT-1             0.0  \n",
       "AAACCTGAGGTCATCT-1             0.0  \n",
       "AAACCTGCAAGAGTCG-1             0.0  \n",
       "AAACCTGCACACATGT-1             0.0  \n",
       "...                            ...  \n",
       "TTTGTCAGTCTTGATG-1             0.0  \n",
       "TTTGTCAGTGATAAAC-1             0.0  \n",
       "TTTGTCATCCTTGACC-1             0.0  \n",
       "TTTGTCATCGGCTACG-1             0.0  \n",
       "TTTGTCATCGTCCGTT-1             0.0  \n",
       "\n",
       "[9128 rows x 27998 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all the cores (12)\n"
     ]
    }
   ],
   "source": [
    "# Using default parameters\n",
    "multinet = MultiNet() \n",
    "\n",
    "# Using custom parameters\n",
    "NN_params = {\n",
    "        'learning_rate': 1e-4,\n",
    "        'batch_size': 64,\n",
    "        'max_epochs': 200,\n",
    "        'ncores': 5,\n",
    "        'sub_outputdim': 512,\n",
    "        'architecture': [\n",
    "            {\"type\": \"dense\", \"activation\": \"relu\", \"neurons\": 200},\n",
    "            {\"type\": \"dropout\", \"activation\": \"dropout\", \"rate\": 0.3}]\n",
    "    }\n",
    "\n",
    "multinet = MultiNet(**NN_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset is 9128 cells (rows) and 27998 genes (columns)\n",
      "First 3 rows and columns:\n",
      "                    Xkr4  Gm1992  Gm37381\n",
      "AAACCTGAGACGCTTT-1   0.0     0.0      0.0\n",
      "AAACCTGAGATCTGCT-1   0.0     0.0      0.0\n",
      "AAACCTGAGGTCATCT-1   0.0     0.0      0.0\n",
      "3072 genes selected for imputation\n",
      "Net 0: 548 predictors, 512 targets\n",
      "Net 1: 546 predictors, 512 targets\n",
      "Net 2: 592 predictors, 512 targets\n",
      "Net 3: 583 predictors, 512 targets\n",
      "Net 4: 544 predictors, 512 targets\n",
      "Net 5: 576 predictors, 512 targets\n",
      "Normalization\n",
      "Building network\n",
      "[{'type': 'dense', 'activation': 'relu', 'neurons': 200}, {'type': 'dropout', 'activation': 'dropout', 'rate': 0.3}]\n",
      "Fitting with 9128 cells\n",
      "Epoch 1/200\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 4.1254 - val_loss: 0.8445\n",
      "Epoch 2/200\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.3226 - val_loss: 0.6740\n",
      "Epoch 3/200\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0355 - val_loss: 0.6155\n",
      "Epoch 4/200\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.8997 - val_loss: 0.5742\n",
      "Epoch 5/200\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.8187 - val_loss: 0.5503\n",
      "Epoch 6/200\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.7660 - val_loss: 0.5240\n",
      "Epoch 7/200\n",
      "\u001b[1m 17/136\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.7280"
     ]
    }
   ],
   "source": [
    "# Using all the data\n",
    "multinet.fit(data,cell_subset=1,minVMR=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imputedData = multinet.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "limits = [0,100]\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "jitter = np.random.normal(0,1,data.size) # Add some jittering to better see the point density\n",
    "ax.scatter(data.values.flatten()+jitter,imputedData.values.flatten(),s=2)\n",
    "ax.plot(limits,limits,'r-.',linewidth=2)\n",
    "ax.set_xlim(limits)\n",
    "ax.set_ylim(limits)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "multinet.test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(score_model(multinet, data, mean_squared_error_metric, cols=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 293T Gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ruta a la carpeta que contiene los archivos 'genes.tsv', 'barcodes.tsv' y 'matrix.mtx'\n",
    "# data_dir = '/Users/juank/Desktop/BCOM/Proyecto/deepimpute/data/293T'\n",
    "data_dir = '/Users/juank/Downloads/filtered_matrices_mex 2/hg19'\n",
    "\n",
    "\n",
    "# Leer los datos usando scanpy\n",
    "adata = sc.read_10x_mtx(data_dir, var_names='gene_symbols', cache=True)\n",
    "\n",
    "# Convertir AnnData a DataFrame de Pandas\n",
    "# Las filas son células y las columnas son genes\n",
    "data = adata.to_df()\n",
    "print('Working on {} cells and {} genes'.format(*data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using default parameters\n",
    "multinet = MultiNet() \n",
    "\n",
    "# Using custom parameters\n",
    "NN_params = {\n",
    "        'learning_rate': 1e-4,\n",
    "        'batch_size': 32,\n",
    "        'max_epochs': 200,\n",
    "        'ncores': 5,\n",
    "        'sub_outputdim': 512,\n",
    "        'architecture': [\n",
    "            {\"type\": \"dense\", \"activation\": \"relu\", \"neurons\": 200},\n",
    "            {\"type\": \"dropout\", \"activation\": \"dropout\", \"rate\": 0.3}]\n",
    "    }\n",
    "\n",
    "multinet = MultiNet(**NN_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using all the data\n",
    "multinet.fit(data,cell_subset=1,minVMR=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imputedData = multinet.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "limits = [0,100]\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "jitter = np.random.normal(0,1,data.size) # Add some jittering to better see the point density\n",
    "ax.scatter(data.values.flatten()+jitter,imputedData.values.flatten(),s=2)\n",
    "ax.plot(limits,limits,'r-.',linewidth=2)\n",
    "ax.set_xlim(limits)\n",
    "ax.set_ylim(limits)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "multinet.test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(score_model(multinet, data, mean_squared_error_metric, cols=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jurkat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ruta a la carpeta que contiene los archivos 'genes.tsv', 'barcodes.tsv' y 'matrix.mtx'\n",
    "data_dir = '/Users/juank/Desktop/BCOM/Proyecto/deepimpute/data/jurkat'\n",
    "\n",
    "\n",
    "# Leer los datos usando scanpy\n",
    "adata = sc.read_10x_mtx(data_dir, var_names='gene_symbols', cache=True)\n",
    "\n",
    "# Convertir AnnData a DataFrame de Pandas\n",
    "# Las filas son células y las columnas son genes\n",
    "data = adata.to_df()\n",
    "print('Working on {} cells and {} genes'.format(*data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using default parameters\n",
    "multinet = MultiNet() \n",
    "\n",
    "# Using custom parameters\n",
    "NN_params = {\n",
    "        'learning_rate': 1e-4,\n",
    "        'batch_size': 32,\n",
    "        'max_epochs': 200,\n",
    "        'ncores': 5,\n",
    "        'sub_outputdim': 512,\n",
    "        'architecture': [\n",
    "            {\"type\": \"dense\", \"activation\": \"relu\", \"neurons\": 200},\n",
    "            {\"type\": \"dropout\", \"activation\": \"dropout\", \"rate\": 0.3}]\n",
    "    }\n",
    "\n",
    "multinet = MultiNet(**NN_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using all the data\n",
    "multinet.fit(data,cell_subset=1,minVMR=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imputedData = multinet.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "limits = [0,100]\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "jitter = np.random.normal(0,1,data.size) # Add some jittering to better see the point density\n",
    "ax.scatter(data.values.flatten()+jitter,imputedData.values.flatten(),s=2)\n",
    "ax.plot(limits,limits,'r-.',linewidth=2)\n",
    "ax.set_xlim(limits)\n",
    "ax.set_ylim(limits)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "multinet.test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(score_model(multinet, data, mean_squared_error_metric, cols=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mouse1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/anndata/_core/anndata.py:1820: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    }
   ],
   "source": [
    "data_path = '/Users/juank/Desktop/BCOM/Proyecto/deepimpute/data/1M_neurons_filtered_gene_bc_matrices_h5.h5'\n",
    "\n",
    "# Cargar los datos usando scanpy\n",
    "adata = sc.read_10x_h5(data_path)\n",
    "\n",
    "# Convertir AnnData a DataFrame de Pandas\n",
    "# Las filas son células y las columnas son genes\n",
    "data = adata.to_df()\n",
    "print('Working on {} cells and {} genes'.format(*data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using default parameters\n",
    "multinet = MultiNet() \n",
    "\n",
    "# Using custom parameters\n",
    "NN_params = {\n",
    "        'learning_rate': 1e-4,\n",
    "        'batch_size': 64,\n",
    "        'max_epochs': 200,\n",
    "        'ncores': 5,\n",
    "        'sub_outputdim': 512,\n",
    "        'architecture': [\n",
    "            {\"type\": \"dense\", \"activation\": \"relu\", \"neurons\": 200},\n",
    "            {\"type\": \"dropout\", \"activation\": \"dropout\", \"rate\": 0.3}]\n",
    "    }\n",
    "\n",
    "multinet = MultiNet(**NN_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using all the data\n",
    "multinet.fit(data,cell_subset=1,minVMR=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputedData = multinet.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limits = [0,100]\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "jitter = np.random.normal(0,1,data.size) # Add some jittering to better see the point density\n",
    "ax.scatter(data.values.flatten()+jitter,imputedData.values.flatten(),s=2)\n",
    "ax.plot(limits,limits,'r-.',linewidth=2)\n",
    "ax.set_xlim(limits)\n",
    "ax.set_ylim(limits)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinet.test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score_model(multinet, data, mean_squared_error_metric, cols=None))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
