{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-cell RNA-seq imputation using DeepImpute\n",
    "\n",
    "Here is a comprehensive tutorial to understand the functionnalities of DeepImpute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 500 cells and 3000 genes\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/juank/Desktop/BCOM/Proyecto/deepimpute')\n",
    "\n",
    "from deepimpute.multinet import MultiNet\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset using pandas\n",
    "data = pd.read_csv('test.csv',index_col=0)\n",
    "print('Working on {} cells and {} genes'.format(*data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1772071015_C02</th>\n",
       "      <th>1772071017_G12</th>\n",
       "      <th>1772071017_A05</th>\n",
       "      <th>1772071014_B06</th>\n",
       "      <th>1772067065_H06</th>\n",
       "      <th>1772071017_E02</th>\n",
       "      <th>1772067065_B07</th>\n",
       "      <th>1772067060_B09</th>\n",
       "      <th>1772071014_E04</th>\n",
       "      <th>1772071015_D04</th>\n",
       "      <th>...</th>\n",
       "      <th>1772067076_E04</th>\n",
       "      <th>1772066097_C06</th>\n",
       "      <th>1772067057_D12</th>\n",
       "      <th>1772066100_A05</th>\n",
       "      <th>1772071015_A11</th>\n",
       "      <th>1772071015_C09</th>\n",
       "      <th>1772062128_F10</th>\n",
       "      <th>1772071017_H06</th>\n",
       "      <th>1772071014_A12</th>\n",
       "      <th>1772063077_H05</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Atp1b2</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sub1</th>\n",
       "      <td>28</td>\n",
       "      <td>41</td>\n",
       "      <td>57</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>49</td>\n",
       "      <td>26</td>\n",
       "      <td>47</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>27</td>\n",
       "      <td>44</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ptprf</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cers5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rit2</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eif1ax</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rbbp7</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trappc2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rab9</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vamp7</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3529 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         1772071015_C02  1772071017_G12  1772071017_A05  1772071014_B06  \\\n",
       "Atp1b2                9               5               8               6   \n",
       "Sub1                 28              41              57              33   \n",
       "Ptprf                 0               0               4               1   \n",
       "Cers5                 0               0               7               1   \n",
       "Rit2                  2               6               3               1   \n",
       "...                 ...             ...             ...             ...   \n",
       "Eif1ax                1               1               2               1   \n",
       "Rbbp7                 1               2               4               4   \n",
       "Trappc2               1               1               2               1   \n",
       "Rab9                  7               1               1               3   \n",
       "Vamp7                 5               0               3               0   \n",
       "\n",
       "         1772067065_H06  1772071017_E02  1772067065_B07  1772067060_B09  \\\n",
       "Atp1b2                7               9               0               3   \n",
       "Sub1                 30              13              27              17   \n",
       "Ptprf                 5               2               0               4   \n",
       "Cers5                 0               1               2               0   \n",
       "Rit2                 12               2               2               2   \n",
       "...                 ...             ...             ...             ...   \n",
       "Eif1ax                1               1               0               0   \n",
       "Rbbp7                 6               4               6               8   \n",
       "Trappc2               2               0               0               0   \n",
       "Rab9                  0               0               2               0   \n",
       "Vamp7                 3               2               8               2   \n",
       "\n",
       "         1772071014_E04  1772071015_D04  ...  1772067076_E04  1772066097_C06  \\\n",
       "Atp1b2                6               4  ...               2               0   \n",
       "Sub1                 23              31  ...              49              26   \n",
       "Ptprf                 0              10  ...               1               1   \n",
       "Cers5                 0               0  ...               3               0   \n",
       "Rit2                 10               7  ...               2               9   \n",
       "...                 ...             ...  ...             ...             ...   \n",
       "Eif1ax                0               1  ...               2               5   \n",
       "Rbbp7                 3               4  ...               1               4   \n",
       "Trappc2               1               1  ...               5               6   \n",
       "Rab9                  0               0  ...               1               2   \n",
       "Vamp7                 3               3  ...               0               3   \n",
       "\n",
       "         1772067057_D12  1772066100_A05  1772071015_A11  1772071015_C09  \\\n",
       "Atp1b2                2               1               2               7   \n",
       "Sub1                 47              16              18              27   \n",
       "Ptprf                 3               0               1               7   \n",
       "Cers5                13               2               0               2   \n",
       "Rit2                  2               0               6               3   \n",
       "...                 ...             ...             ...             ...   \n",
       "Eif1ax                4               2               0               0   \n",
       "Rbbp7                 4               0               2               2   \n",
       "Trappc2               5               2               1               2   \n",
       "Rab9                  3               2               0               6   \n",
       "Vamp7                 0               0               0               0   \n",
       "\n",
       "         1772062128_F10  1772071017_H06  1772071014_A12  1772063077_H05  \n",
       "Atp1b2                1              10               3               1  \n",
       "Sub1                 44               5              23              18  \n",
       "Ptprf                 7               0               2               0  \n",
       "Cers5                 3               3               4               1  \n",
       "Rit2                  5              11               2               5  \n",
       "...                 ...             ...             ...             ...  \n",
       "Eif1ax                1               1               3               3  \n",
       "Rbbp7                 0               2               1               0  \n",
       "Trappc2               1               0               4               0  \n",
       "Rab9                  2               2               2               3  \n",
       "Vamp7                 2               1               2               1  \n",
       "\n",
       "[3529 rows x 200 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/juank/Desktop/BCOM/Proyecto/deepimpute/data/linnarsson.csv',index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ENSG00000177954</th>\n",
       "      <th>ENSG00000197756</th>\n",
       "      <th>ENSG00000231500</th>\n",
       "      <th>ENSG00000140988</th>\n",
       "      <th>ENSG00000105372</th>\n",
       "      <th>ENSG00000198712</th>\n",
       "      <th>ENSG00000109475</th>\n",
       "      <th>ENSG00000112306</th>\n",
       "      <th>ENSG00000137818</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000269858</th>\n",
       "      <th>ENSG00000182087</th>\n",
       "      <th>ENSG00000160214</th>\n",
       "      <th>ENSG00000166411</th>\n",
       "      <th>ENSG00000186153</th>\n",
       "      <th>ENSG00000089351</th>\n",
       "      <th>ENSG00000108433</th>\n",
       "      <th>ENSG00000206053</th>\n",
       "      <th>ENSG00000137806</th>\n",
       "      <th>ENSG00000197766</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AATTGTGACTACGA-1</td>\n",
       "      <td>826.0</td>\n",
       "      <td>674.0</td>\n",
       "      <td>694.0</td>\n",
       "      <td>809.0</td>\n",
       "      <td>771.0</td>\n",
       "      <td>796.0</td>\n",
       "      <td>755.0</td>\n",
       "      <td>684.0</td>\n",
       "      <td>597.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TGACACGATTCGTT-1</td>\n",
       "      <td>617.0</td>\n",
       "      <td>618.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>703.0</td>\n",
       "      <td>671.0</td>\n",
       "      <td>473.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TGTCAGGATTGTCT-1</td>\n",
       "      <td>525.0</td>\n",
       "      <td>550.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>615.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>422.0</td>\n",
       "      <td>449.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TAAGTAACGTTCTT-1</td>\n",
       "      <td>514.0</td>\n",
       "      <td>474.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>447.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCCTAAACTTCATC-1</td>\n",
       "      <td>444.0</td>\n",
       "      <td>507.0</td>\n",
       "      <td>509.0</td>\n",
       "      <td>566.0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>352.0</td>\n",
       "      <td>413.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>AAATGGGATGCCTC-1</td>\n",
       "      <td>169.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>TAAAGTTGTTACCT-1</td>\n",
       "      <td>202.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>AGCGAACTCTAGTG-1</td>\n",
       "      <td>160.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>GTCACCTGTTACCT-1</td>\n",
       "      <td>140.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>TGAGTGACTACTGG-1</td>\n",
       "      <td>149.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 3001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Unnamed: 0  ENSG00000177954  ENSG00000197756  ENSG00000231500  \\\n",
       "0    AATTGTGACTACGA-1            826.0            674.0            694.0   \n",
       "1    TGACACGATTCGTT-1            617.0            618.0            594.0   \n",
       "2    TGTCAGGATTGTCT-1            525.0            550.0            540.0   \n",
       "3    TAAGTAACGTTCTT-1            514.0            474.0            361.0   \n",
       "4    TCCTAAACTTCATC-1            444.0            507.0            509.0   \n",
       "..                ...              ...              ...              ...   \n",
       "495  AAATGGGATGCCTC-1            169.0            149.0            161.0   \n",
       "496  TAAAGTTGTTACCT-1            202.0            216.0            177.0   \n",
       "497  AGCGAACTCTAGTG-1            160.0            154.0            184.0   \n",
       "498  GTCACCTGTTACCT-1            140.0            133.0            179.0   \n",
       "499  TGAGTGACTACTGG-1            149.0            120.0            135.0   \n",
       "\n",
       "     ENSG00000140988  ENSG00000105372  ENSG00000198712  ENSG00000109475  \\\n",
       "0              809.0            771.0            796.0            755.0   \n",
       "1              703.0            671.0            473.0            549.0   \n",
       "2              546.0            615.0            565.0            263.0   \n",
       "3              331.0            447.0            279.0            188.0   \n",
       "4              566.0            520.0            246.0            352.0   \n",
       "..               ...              ...              ...              ...   \n",
       "495            203.0            178.0            155.0            128.0   \n",
       "496            253.0            257.0             34.0             38.0   \n",
       "497            253.0            150.0            190.0            101.0   \n",
       "498            246.0            197.0             89.0             88.0   \n",
       "499            173.0            177.0            182.0            147.0   \n",
       "\n",
       "     ENSG00000112306  ENSG00000137818  ...  ENSG00000269858  ENSG00000182087  \\\n",
       "0              684.0            597.0  ...              3.0              2.0   \n",
       "1              523.0            476.0  ...              2.0              1.0   \n",
       "2              422.0            449.0  ...              2.0              1.0   \n",
       "3              332.0            379.0  ...              1.0              2.0   \n",
       "4              413.0            381.0  ...              1.0              2.0   \n",
       "..               ...              ...  ...              ...              ...   \n",
       "495            150.0            124.0  ...              0.0              3.0   \n",
       "496            188.0            137.0  ...              1.0              0.0   \n",
       "497            191.0            158.0  ...              0.0              0.0   \n",
       "498            150.0            153.0  ...              2.0              1.0   \n",
       "499            110.0            128.0  ...              1.0              0.0   \n",
       "\n",
       "     ENSG00000160214  ENSG00000166411  ENSG00000186153  ENSG00000089351  \\\n",
       "0                3.0              2.0              2.0              0.0   \n",
       "1                2.0              2.0              3.0              3.0   \n",
       "2                0.0              3.0              1.0              2.0   \n",
       "3                0.0              3.0              3.0              0.0   \n",
       "4                0.0              5.0              4.0              0.0   \n",
       "..               ...              ...              ...              ...   \n",
       "495              1.0              1.0              0.0              0.0   \n",
       "496              1.0              0.0              1.0              1.0   \n",
       "497              0.0              1.0              0.0              0.0   \n",
       "498              1.0              3.0              1.0              0.0   \n",
       "499              1.0              0.0              1.0              2.0   \n",
       "\n",
       "     ENSG00000108433  ENSG00000206053  ENSG00000137806  ENSG00000197766  \n",
       "0                0.0              1.0              5.0              4.0  \n",
       "1                1.0              4.0              2.0              3.0  \n",
       "2                3.0              2.0              2.0              1.0  \n",
       "3                1.0              0.0              3.0              1.0  \n",
       "4                3.0              4.0              2.0              1.0  \n",
       "..               ...              ...              ...              ...  \n",
       "495              0.0              0.0              0.0              1.0  \n",
       "496              0.0              1.0              2.0              1.0  \n",
       "497              0.0              1.0              2.0              2.0  \n",
       "498              0.0              0.0              3.0              1.0  \n",
       "499              0.0              0.0              1.0              1.0  \n",
       "\n",
       "[500 rows x 3001 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('test.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>^DATABASE = GeoMiame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!Database_name = Gene Expression Omnibus (GEO)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!Database_institute = NCBI NLM NIH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>!Database_web_link = http://www.ncbi.nlm.nih.g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>!Database_email = geo@ncbi.nlm.nih.gov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>^SERIES = GSE99330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37697</th>\n",
       "      <td>!Sample_relation = BioSample: https://www.ncbi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37698</th>\n",
       "      <td>!Sample_relation = SRA: https://www.ncbi.nlm.n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37699</th>\n",
       "      <td>!Sample_supplementary_file_1 = NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37700</th>\n",
       "      <td>!Sample_series_id = GSE99330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37701</th>\n",
       "      <td>!Sample_data_row_count = 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37702 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    ^DATABASE = GeoMiame\n",
       "0         !Database_name = Gene Expression Omnibus (GEO)\n",
       "1                     !Database_institute = NCBI NLM NIH\n",
       "2      !Database_web_link = http://www.ncbi.nlm.nih.g...\n",
       "3                 !Database_email = geo@ncbi.nlm.nih.gov\n",
       "4                                     ^SERIES = GSE99330\n",
       "...                                                  ...\n",
       "37697  !Sample_relation = BioSample: https://www.ncbi...\n",
       "37698  !Sample_relation = SRA: https://www.ncbi.nlm.n...\n",
       "37699                !Sample_supplementary_file_1 = NONE\n",
       "37700                       !Sample_series_id = GSE99330\n",
       "37701                         !Sample_data_row_count = 0\n",
       "\n",
       "[37702 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fish = pd.read_csv('/Users/juank/Downloads/GSE99330_family.soft', sep=\"\\t\", comment=\"#\", low_memory=False)\n",
    "fish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DeepImpute multinet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all the cores (12)\n"
     ]
    }
   ],
   "source": [
    "# Using default parameters\n",
    "multinet = MultiNet() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using custom parameters\n",
    "NN_params = {\n",
    "        'learning_rate': 1e-4,\n",
    "        'batch_size': 64,\n",
    "        'max_epochs': 200,\n",
    "        'ncores': 5,\n",
    "        'sub_outputdim': 512,\n",
    "        'architecture': [\n",
    "            {\"type\": \"dense\", \"activation\": \"relu\", \"neurons\": 200},\n",
    "            {\"type\": \"dropout\", \"activation\": \"dropout\", \"rate\": 0.3}]\n",
    "    }\n",
    "\n",
    "multinet = MultiNet(**NN_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset is 500 cells (rows) and 3000 genes (columns)\n",
      "First 3 rows and columns:\n",
      "                  ENSG00000177954  ENSG00000197756  ENSG00000231500\n",
      "AATTGTGACTACGA-1            826.0            674.0            694.0\n",
      "TGACACGATTCGTT-1            617.0            618.0            594.0\n",
      "TGTCAGGATTGTCT-1            525.0            550.0            540.0\n",
      "3072 genes selected for imputation\n",
      "Net 0: 639 predictors, 512 targets\n",
      "Net 1: 593 predictors, 512 targets\n",
      "Net 2: 591 predictors, 512 targets\n",
      "Net 3: 594 predictors, 512 targets\n",
      "Net 4: 555 predictors, 512 targets\n",
      "Net 5: 631 predictors, 512 targets\n",
      "Normalization\n",
      "Building network\n",
      "[{'type': 'dense', 'activation': 'relu', 'neurons': 200}, {'type': 'dropout', 'activation': 'dropout', 'rate': 0.3}]\n",
      "Fitting with 500 cells\n",
      "Epoch 1/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 29.8549 - val_loss: 24.6863\n",
      "Epoch 2/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 23.9448 - val_loss: 19.4177\n",
      "Epoch 3/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 19.7779 - val_loss: 15.6319\n",
      "Epoch 4/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 16.8571 - val_loss: 12.9088\n",
      "Epoch 5/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.7572 - val_loss: 10.9206\n",
      "Epoch 6/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.2004 - val_loss: 9.3731\n",
      "Epoch 7/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 11.9207 - val_loss: 8.1287\n",
      "Epoch 8/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10.8760 - val_loss: 7.0732\n",
      "Epoch 9/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10.0245 - val_loss: 6.2002\n",
      "Epoch 10/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.2492 - val_loss: 5.4707\n",
      "Epoch 11/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.6088 - val_loss: 4.8733\n",
      "Epoch 12/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.0628 - val_loss: 4.3547\n",
      "Epoch 13/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.5799 - val_loss: 3.9760\n",
      "Epoch 14/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.1340 - val_loss: 3.6070\n",
      "Epoch 15/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.7850 - val_loss: 3.3712\n",
      "Epoch 16/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4855 - val_loss: 3.1410\n",
      "Epoch 17/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.2807 - val_loss: 2.9395\n",
      "Epoch 18/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.9598 - val_loss: 2.8608\n",
      "Epoch 19/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.7967 - val_loss: 2.6810\n",
      "Epoch 20/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.6149 - val_loss: 2.5844\n",
      "Epoch 21/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.4021 - val_loss: 2.4969\n",
      "Epoch 22/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.2463 - val_loss: 2.4466\n",
      "Epoch 23/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.1306 - val_loss: 2.3225\n",
      "Epoch 24/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.0194 - val_loss: 2.3346\n",
      "Epoch 25/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.9554 - val_loss: 2.2140\n",
      "Epoch 26/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.8373 - val_loss: 2.1447\n",
      "Epoch 27/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6933 - val_loss: 2.2021\n",
      "Epoch 28/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.5722 - val_loss: 2.0738\n",
      "Epoch 29/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.4869 - val_loss: 2.1291\n",
      "Epoch 30/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.4394 - val_loss: 2.0508\n",
      "Epoch 31/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.3058 - val_loss: 2.0368\n",
      "Epoch 32/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.2494 - val_loss: 2.0265\n",
      "Epoch 33/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.1911 - val_loss: 1.9499\n",
      "Epoch 34/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.1256 - val_loss: 1.9398\n",
      "Epoch 35/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.0719 - val_loss: 1.9632\n",
      "Epoch 36/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.9966 - val_loss: 1.8840\n",
      "Epoch 37/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.9527 - val_loss: 1.8764\n",
      "Epoch 38/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.8353 - val_loss: 1.8204\n",
      "Epoch 39/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8226 - val_loss: 1.8713\n",
      "Epoch 40/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.7689 - val_loss: 1.8553\n",
      "Epoch 41/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.7174 - val_loss: 1.8428\n",
      "Epoch 42/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.6393 - val_loss: 1.7830\n",
      "Epoch 43/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.5671 - val_loss: 1.8158\n",
      "Epoch 44/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5972 - val_loss: 1.7844\n",
      "Epoch 45/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5220 - val_loss: 1.8235\n",
      "Epoch 46/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5055 - val_loss: 1.7451\n",
      "Epoch 47/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.4475 - val_loss: 1.7255\n",
      "Epoch 48/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.4404 - val_loss: 1.7670\n",
      "Epoch 49/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.3681 - val_loss: 1.7431\n",
      "Epoch 50/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.3187 - val_loss: 1.6736\n",
      "Epoch 51/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.3185 - val_loss: 1.6988\n",
      "Epoch 52/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.2613 - val_loss: 1.6750\n",
      "Epoch 53/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.2430 - val_loss: 1.6716\n",
      "Epoch 54/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.2294 - val_loss: 1.6725\n",
      "Epoch 55/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.1710 - val_loss: 1.6491\n",
      "Epoch 56/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.1742 - val_loss: 1.6945\n",
      "Epoch 57/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.1144 - val_loss: 1.6236\n",
      "Epoch 58/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.1113 - val_loss: 1.6392\n",
      "Epoch 59/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0719 - val_loss: 1.6714\n",
      "Epoch 60/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0795 - val_loss: 1.6428\n",
      "Epoch 61/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0179 - val_loss: 1.6609\n",
      "Epoch 62/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.9845 - val_loss: 1.6031\n",
      "Epoch 63/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.9716 - val_loss: 1.6313\n",
      "Epoch 64/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9746 - val_loss: 1.6589\n",
      "Epoch 65/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.9175 - val_loss: 1.5702\n",
      "Epoch 66/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.9005 - val_loss: 1.6039\n",
      "Epoch 67/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8766 - val_loss: 1.6405\n",
      "Epoch 68/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.8748 - val_loss: 1.6744\n",
      "Epoch 69/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8447 - val_loss: 1.5228\n",
      "Epoch 70/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8155 - val_loss: 1.5178\n",
      "Epoch 71/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8074 - val_loss: 1.5406\n",
      "Epoch 72/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7868 - val_loss: 1.6106\n",
      "Epoch 73/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7905 - val_loss: 1.6684\n",
      "Epoch 74/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7832 - val_loss: 1.6400\n",
      "Epoch 75/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7642 - val_loss: 1.5537\n",
      "Stopped fitting after 75 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk in /var/folders/t1/fw_bh5nj2sg1zsn8ft2cld2c0000gn/T/tmpt3c3oytw\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepimpute.multinet.MultiNet at 0x104001a90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using all the data\n",
    "multinet.fit(data,cell_subset=1,minVMR=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset is 500 cells (rows) and 3000 genes (columns)\n",
      "First 3 rows and columns:\n",
      "                  ENSG00000177954  ENSG00000197756  ENSG00000231500\n",
      "AATTGTGACTACGA-1            826.0            674.0            694.0\n",
      "TGACACGATTCGTT-1            617.0            618.0            594.0\n",
      "TGTCAGGATTGTCT-1            525.0            550.0            540.0\n",
      "3072 genes selected for imputation\n",
      "Net 0: 847 predictors, 512 targets\n",
      "Net 1: 839 predictors, 512 targets\n",
      "Net 2: 842 predictors, 512 targets\n",
      "Net 3: 840 predictors, 512 targets\n",
      "Net 4: 820 predictors, 512 targets\n",
      "Net 5: 766 predictors, 512 targets\n",
      "Normalization\n",
      "Building network\n",
      "[{'type': 'dense', 'activation': 'relu', 'neurons': 200}, {'type': 'dropout', 'activation': 'dropout', 'rate': 0.3}]\n",
      "Fitting with 250 cells\n",
      "Epoch 1/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 30.1276 - val_loss: 25.0546\n",
      "Epoch 2/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 26.7142 - val_loss: 21.8920\n",
      "Epoch 3/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 23.7704 - val_loss: 19.1379\n",
      "Epoch 4/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 21.4550 - val_loss: 16.9067\n",
      "Epoch 5/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 19.5930 - val_loss: 15.1485\n",
      "Epoch 6/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 18.1453 - val_loss: 13.7088\n",
      "Epoch 7/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 16.8479 - val_loss: 12.4962\n",
      "Epoch 8/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15.6968 - val_loss: 11.4763\n",
      "Epoch 9/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14.7484 - val_loss: 10.5920\n",
      "Epoch 10/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13.9087 - val_loss: 9.7780\n",
      "Epoch 11/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13.1547 - val_loss: 9.0155\n",
      "Epoch 12/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.5062 - val_loss: 8.3238\n",
      "Epoch 13/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.9190 - val_loss: 7.7024\n",
      "Epoch 14/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.3743 - val_loss: 7.1397\n",
      "Epoch 15/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.8473 - val_loss: 6.6395\n",
      "Epoch 16/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 10.4332 - val_loss: 6.2066\n",
      "Epoch 17/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.9408 - val_loss: 5.8124\n",
      "Epoch 18/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.6325 - val_loss: 5.4140\n",
      "Epoch 19/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.1232 - val_loss: 5.0533\n",
      "Epoch 20/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.7930 - val_loss: 4.7507\n",
      "Epoch 21/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.6057 - val_loss: 4.4819\n",
      "Epoch 22/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.2160 - val_loss: 4.2474\n",
      "Epoch 23/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.9610 - val_loss: 4.0081\n",
      "Epoch 24/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.7914 - val_loss: 3.7890\n",
      "Epoch 25/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.5306 - val_loss: 3.6118\n",
      "Epoch 26/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.3696 - val_loss: 3.4296\n",
      "Epoch 27/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.0645 - val_loss: 3.2884\n",
      "Epoch 28/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.9646 - val_loss: 3.1563\n",
      "Epoch 29/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.7614 - val_loss: 3.0327\n",
      "Epoch 30/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.5669 - val_loss: 2.9462\n",
      "Epoch 31/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.4008 - val_loss: 2.8063\n",
      "Epoch 32/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.3119 - val_loss: 2.7455\n",
      "Epoch 33/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.3365 - val_loss: 2.6607\n",
      "Epoch 34/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.0435 - val_loss: 2.5560\n",
      "Epoch 35/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 5.9730 - val_loss: 2.5088\n",
      "Epoch 36/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.7927 - val_loss: 2.5111\n",
      "Epoch 37/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.7551 - val_loss: 2.3908\n",
      "Epoch 38/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.5409 - val_loss: 2.3336\n",
      "Epoch 39/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.4776 - val_loss: 2.3304\n",
      "Epoch 40/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.4708 - val_loss: 2.2410\n",
      "Epoch 41/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.3788 - val_loss: 2.1720\n",
      "Epoch 42/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.3305 - val_loss: 2.1703\n",
      "Epoch 43/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.2014 - val_loss: 2.1809\n",
      "Epoch 44/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.1917 - val_loss: 2.0941\n",
      "Epoch 45/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.0752 - val_loss: 2.0871\n",
      "Epoch 46/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.9887 - val_loss: 2.1037\n",
      "Epoch 47/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.9722 - val_loss: 2.0105\n",
      "Epoch 48/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.9073 - val_loss: 1.9884\n",
      "Epoch 49/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.8343 - val_loss: 2.0201\n",
      "Epoch 50/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.8030 - val_loss: 1.9748\n",
      "Epoch 51/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.8082 - val_loss: 1.9166\n",
      "Epoch 52/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.7126 - val_loss: 1.9161\n",
      "Epoch 53/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.6527 - val_loss: 1.9230\n",
      "Epoch 54/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.5669 - val_loss: 1.9056\n",
      "Epoch 55/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.5007 - val_loss: 1.9314\n",
      "Epoch 56/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.5989 - val_loss: 1.8151\n",
      "Epoch 57/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.4306 - val_loss: 1.8564\n",
      "Epoch 58/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.4034 - val_loss: 1.8830\n",
      "Epoch 59/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.3710 - val_loss: 1.7880\n",
      "Epoch 60/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.2798 - val_loss: 1.8477\n",
      "Epoch 61/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.2730 - val_loss: 1.8197\n",
      "Epoch 62/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.2372 - val_loss: 1.7856\n",
      "Epoch 63/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.2241 - val_loss: 1.7839\n",
      "Epoch 64/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.1566 - val_loss: 1.7824\n",
      "Epoch 65/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.1067 - val_loss: 1.7480\n",
      "Epoch 66/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.0585 - val_loss: 1.7925\n",
      "Epoch 67/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.0652 - val_loss: 1.6904\n",
      "Epoch 68/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.0674 - val_loss: 1.7059\n",
      "Epoch 69/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.9991 - val_loss: 1.7315\n",
      "Epoch 70/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.0084 - val_loss: 1.6763\n",
      "Epoch 71/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.9527 - val_loss: 1.7091\n",
      "Epoch 72/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.8817 - val_loss: 1.7307\n",
      "Epoch 73/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.8917 - val_loss: 1.6348\n",
      "Epoch 74/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.8828 - val_loss: 1.7228\n",
      "Epoch 75/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.8049 - val_loss: 1.6624\n",
      "Epoch 76/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.8137 - val_loss: 1.6022\n",
      "Epoch 77/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.7596 - val_loss: 1.7817\n",
      "Epoch 78/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.6965 - val_loss: 1.6005\n",
      "Epoch 79/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.7512 - val_loss: 1.6053\n",
      "Epoch 80/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.6904 - val_loss: 1.6601\n",
      "Epoch 81/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.7093 - val_loss: 1.6194\n",
      "Epoch 82/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.6430 - val_loss: 1.6510\n",
      "Epoch 83/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.5674 - val_loss: 1.6474\n",
      "Stopped fitting after 83 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk in /var/folders/t1/fw_bh5nj2sg1zsn8ft2cld2c0000gn/T/tmpt3c3oytw\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepimpute.multinet.MultiNet at 0x104001a90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using 80% of the data\n",
    "multinet.fit(data,cell_subset=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset is 500 cells (rows) and 3000 genes (columns)\n",
      "First 3 rows and columns:\n",
      "                  ENSG00000177954  ENSG00000197756  ENSG00000231500\n",
      "AATTGTGACTACGA-1            826.0            674.0            694.0\n",
      "TGACACGATTCGTT-1            617.0            618.0            594.0\n",
      "TGTCAGGATTGTCT-1            525.0            550.0            540.0\n",
      "3072 genes selected for imputation\n",
      "Net 0: 907 predictors, 512 targets\n",
      "Net 1: 912 predictors, 512 targets\n",
      "Net 2: 905 predictors, 512 targets\n",
      "Net 3: 923 predictors, 512 targets\n",
      "Net 4: 898 predictors, 512 targets\n",
      "Net 5: 848 predictors, 512 targets\n",
      "Normalization\n",
      "Building network\n",
      "[{'type': 'dense', 'activation': 'relu', 'neurons': 200}, {'type': 'dropout', 'activation': 'dropout', 'rate': 0.3}]\n",
      "Fitting with 200 cells\n",
      "Epoch 1/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 29.5517 - val_loss: 24.5187\n",
      "Epoch 2/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 26.7996 - val_loss: 22.2365\n",
      "Epoch 3/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 24.4739 - val_loss: 20.0696\n",
      "Epoch 4/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 22.4037 - val_loss: 18.0961\n",
      "Epoch 5/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 20.6651 - val_loss: 16.3833\n",
      "Epoch 6/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 19.2116 - val_loss: 14.9243\n",
      "Epoch 7/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 17.9828 - val_loss: 13.7080\n",
      "Epoch 8/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 16.9935 - val_loss: 12.6996\n",
      "Epoch 9/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 16.2586 - val_loss: 11.8674\n",
      "Epoch 10/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 15.4105 - val_loss: 11.1660\n",
      "Epoch 11/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 14.8157 - val_loss: 10.5510\n",
      "Epoch 12/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 14.1864 - val_loss: 9.9892\n",
      "Epoch 13/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 13.7486 - val_loss: 9.4496\n",
      "Epoch 14/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 13.2262 - val_loss: 8.9361\n",
      "Epoch 15/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 12.6912 - val_loss: 8.4380\n",
      "Epoch 16/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 12.2952 - val_loss: 7.9641\n",
      "Epoch 17/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 11.8895 - val_loss: 7.5182\n",
      "Epoch 18/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 11.4882 - val_loss: 7.1222\n",
      "Epoch 19/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 11.1410 - val_loss: 6.7723\n",
      "Epoch 20/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 10.8602 - val_loss: 6.4389\n",
      "Epoch 21/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 10.5674 - val_loss: 6.1150\n",
      "Epoch 22/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 10.1189 - val_loss: 5.8228\n",
      "Epoch 23/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.8602 - val_loss: 5.5564\n",
      "Epoch 24/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.5630 - val_loss: 5.2999\n",
      "Epoch 25/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.3352 - val_loss: 5.0676\n",
      "Epoch 26/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.0263 - val_loss: 4.8580\n",
      "Epoch 27/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.9137 - val_loss: 4.6425\n",
      "Epoch 28/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.6275 - val_loss: 4.4509\n",
      "Epoch 29/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.5213 - val_loss: 4.2776\n",
      "Epoch 30/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.2579 - val_loss: 4.1103\n",
      "Epoch 31/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.0632 - val_loss: 3.9548\n",
      "Epoch 32/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.8880 - val_loss: 3.8185\n",
      "Epoch 33/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 7.7180 - val_loss: 3.6965\n",
      "Epoch 34/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.5705 - val_loss: 3.5833\n",
      "Epoch 35/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.4940 - val_loss: 3.4610\n",
      "Epoch 36/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.3238 - val_loss: 3.3485\n",
      "Epoch 37/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.2277 - val_loss: 3.2484\n",
      "Epoch 38/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.9901 - val_loss: 3.1518\n",
      "Epoch 39/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 6.9404 - val_loss: 3.0988\n",
      "Epoch 40/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 6.9038 - val_loss: 3.0099\n",
      "Epoch 41/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 6.6300 - val_loss: 2.9060\n",
      "Epoch 42/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 6.6101 - val_loss: 2.8338\n",
      "Epoch 43/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 6.4857 - val_loss: 2.7724\n",
      "Epoch 44/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 6.3329 - val_loss: 2.6961\n",
      "Epoch 45/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 6.2490 - val_loss: 2.6226\n",
      "Epoch 46/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.1211 - val_loss: 2.6030\n",
      "Epoch 47/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 6.0423 - val_loss: 2.5935\n",
      "Epoch 48/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.9685 - val_loss: 2.5267\n",
      "Epoch 49/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 5.9189 - val_loss: 2.4493\n",
      "Epoch 50/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.8355 - val_loss: 2.3894\n",
      "Epoch 51/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 5.7180 - val_loss: 2.3781\n",
      "Epoch 52/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 5.7145 - val_loss: 2.3280\n",
      "Epoch 53/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 5.5748 - val_loss: 2.2944\n",
      "Epoch 54/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.6430 - val_loss: 2.2279\n",
      "Epoch 55/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.4963 - val_loss: 2.2100\n",
      "Epoch 56/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.4529 - val_loss: 2.2171\n",
      "Epoch 57/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.3469 - val_loss: 2.2016\n",
      "Epoch 58/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.4571 - val_loss: 2.1222\n",
      "Epoch 59/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.3151 - val_loss: 2.0515\n",
      "Epoch 60/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.2654 - val_loss: 2.0663\n",
      "Epoch 61/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.1542 - val_loss: 2.0808\n",
      "Epoch 62/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.1158 - val_loss: 2.0399\n",
      "Epoch 63/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.0647 - val_loss: 1.9795\n",
      "Epoch 64/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.0128 - val_loss: 1.9989\n",
      "Epoch 65/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.0056 - val_loss: 1.9992\n",
      "Epoch 66/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.9101 - val_loss: 1.9582\n",
      "Epoch 67/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.9189 - val_loss: 1.9281\n",
      "Epoch 68/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.8337 - val_loss: 1.9108\n",
      "Epoch 69/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.8298 - val_loss: 1.8848\n",
      "Epoch 70/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.7522 - val_loss: 1.9009\n",
      "Epoch 71/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.7402 - val_loss: 1.8821\n",
      "Epoch 72/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.6136 - val_loss: 1.9000\n",
      "Epoch 73/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.6399 - val_loss: 1.8652\n",
      "Epoch 74/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.5814 - val_loss: 1.8476\n",
      "Epoch 75/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.6407 - val_loss: 1.7903\n",
      "Epoch 76/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.4963 - val_loss: 1.8099\n",
      "Epoch 77/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.5288 - val_loss: 1.8350\n",
      "Epoch 78/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.5027 - val_loss: 1.7710\n",
      "Epoch 79/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.4454 - val_loss: 1.7327\n",
      "Epoch 80/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.4223 - val_loss: 1.7850\n",
      "Epoch 81/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.4005 - val_loss: 1.7879\n",
      "Epoch 82/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.3435 - val_loss: 1.7326\n",
      "Epoch 83/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.3256 - val_loss: 1.7248\n",
      "Epoch 84/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.2655 - val_loss: 1.7339\n",
      "Epoch 85/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.2834 - val_loss: 1.7027\n",
      "Epoch 86/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.2618 - val_loss: 1.6804\n",
      "Epoch 87/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.1743 - val_loss: 1.7151\n",
      "Epoch 88/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.1990 - val_loss: 1.7410\n",
      "Epoch 89/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.1686 - val_loss: 1.6660\n",
      "Epoch 90/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.1385 - val_loss: 1.6422\n",
      "Epoch 91/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.1292 - val_loss: 1.7127\n",
      "Epoch 92/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.0815 - val_loss: 1.6685\n",
      "Epoch 93/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.0866 - val_loss: 1.6132\n",
      "Epoch 94/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.0500 - val_loss: 1.6400\n",
      "Epoch 95/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.9560 - val_loss: 1.6932\n",
      "Epoch 96/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.9899 - val_loss: 1.6377\n",
      "Epoch 97/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.9708 - val_loss: 1.5577\n",
      "Epoch 98/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.9672 - val_loss: 1.6138\n",
      "Epoch 99/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.8666 - val_loss: 1.7248\n",
      "Epoch 100/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.9477 - val_loss: 1.6138\n",
      "Epoch 101/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.8827 - val_loss: 1.5062\n",
      "Epoch 102/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.8258 - val_loss: 1.5936\n",
      "Epoch 103/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.8140 - val_loss: 1.7165\n",
      "Epoch 104/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.7915 - val_loss: 1.5491\n",
      "Epoch 105/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.8210 - val_loss: 1.4884\n",
      "Epoch 106/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.7799 - val_loss: 1.6032\n",
      "Epoch 107/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.7928 - val_loss: 1.5939\n",
      "Epoch 108/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.7376 - val_loss: 1.5548\n",
      "Epoch 109/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.6978 - val_loss: 1.5815\n",
      "Epoch 110/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.6517 - val_loss: 1.5664\n",
      "Stopped fitting after 110 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk in /var/folders/t1/fw_bh5nj2sg1zsn8ft2cld2c0000gn/T/tmpt3c3oytw\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepimpute.multinet.MultiNet at 0x104001a90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using 200 cells (randomly selected)\n",
    "multinet.fit(data,cell_subset=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset is 150 cells (rows) and 3000 genes (columns)\n",
      "First 3 rows and columns:\n",
      "                  ENSG00000177954  ENSG00000197756  ENSG00000231500\n",
      "AATACCCTGGGACA-1            271.0            262.0            231.0\n",
      "GGCGCATGCCTAAG-1            173.0            390.0            358.0\n",
      "CGCACTTGAACCAC-1            367.0            406.0            354.0\n",
      "3072 genes selected for imputation\n",
      "Net 0: 1252 predictors, 512 targets\n",
      "Net 1: 1263 predictors, 512 targets\n",
      "Net 2: 1250 predictors, 512 targets\n",
      "Net 3: 1273 predictors, 512 targets\n",
      "Net 4: 1253 predictors, 512 targets\n",
      "Net 5: 1251 predictors, 512 targets\n",
      "Normalization\n",
      "Building network\n",
      "[{'type': 'dense', 'activation': 'relu', 'neurons': 200}, {'type': 'dropout', 'activation': 'dropout', 'rate': 0.3}]\n",
      "Fitting with 150 cells\n",
      "Epoch 1/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 29.6420 - val_loss: 26.1024\n",
      "Epoch 2/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 26.4871 - val_loss: 23.0008\n",
      "Epoch 3/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23.7962 - val_loss: 20.2696\n",
      "Epoch 4/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 21.5475 - val_loss: 18.0390\n",
      "Epoch 5/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 19.8768 - val_loss: 16.2757\n",
      "Epoch 6/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 18.5481 - val_loss: 14.8663\n",
      "Epoch 7/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 17.4442 - val_loss: 13.7240\n",
      "Epoch 8/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 16.5234 - val_loss: 12.7893\n",
      "Epoch 9/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 15.8236 - val_loss: 12.0200\n",
      "Epoch 10/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 15.0673 - val_loss: 11.3570\n",
      "Epoch 11/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 14.4432 - val_loss: 10.7661\n",
      "Epoch 12/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 13.9057 - val_loss: 10.2117\n",
      "Epoch 13/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 13.4993 - val_loss: 9.6788\n",
      "Epoch 14/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 13.0712 - val_loss: 9.1680\n",
      "Epoch 15/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 12.5924 - val_loss: 8.6860\n",
      "Epoch 16/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 12.1326 - val_loss: 8.2300\n",
      "Epoch 17/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 11.8374 - val_loss: 7.8040\n",
      "Epoch 18/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 11.3998 - val_loss: 7.4200\n",
      "Epoch 19/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 11.0642 - val_loss: 7.0488\n",
      "Epoch 20/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 10.8423 - val_loss: 6.7087\n",
      "Epoch 21/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 10.5062 - val_loss: 6.3795\n",
      "Epoch 22/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 10.1983 - val_loss: 6.0763\n",
      "Epoch 23/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.8823 - val_loss: 5.8084\n",
      "Epoch 24/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.5823 - val_loss: 5.5782\n",
      "Epoch 25/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.4004 - val_loss: 5.3683\n",
      "Epoch 26/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.1286 - val_loss: 5.1579\n",
      "Epoch 27/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.9339 - val_loss: 4.9423\n",
      "Epoch 28/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.6474 - val_loss: 4.7337\n",
      "Epoch 29/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.5474 - val_loss: 4.5411\n",
      "Epoch 30/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.2252 - val_loss: 4.3481\n",
      "Epoch 31/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.1767 - val_loss: 4.1860\n",
      "Epoch 32/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.9214 - val_loss: 4.0290\n",
      "Epoch 33/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.7494 - val_loss: 3.8944\n",
      "Epoch 34/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.5641 - val_loss: 3.7478\n",
      "Epoch 35/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.5026 - val_loss: 3.6421\n",
      "Epoch 36/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.2740 - val_loss: 3.5634\n",
      "Epoch 37/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.3129 - val_loss: 3.4587\n",
      "Epoch 38/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.0416 - val_loss: 3.3512\n",
      "Epoch 39/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.9849 - val_loss: 3.2347\n",
      "Epoch 40/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.8547 - val_loss: 3.1287\n",
      "Epoch 41/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.6058 - val_loss: 3.1197\n",
      "Epoch 42/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6.6298 - val_loss: 3.0489\n",
      "Epoch 43/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.4270 - val_loss: 2.9295\n",
      "Epoch 44/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.3740 - val_loss: 2.8524\n",
      "Epoch 45/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6.3109 - val_loss: 2.7878\n",
      "Epoch 46/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.2012 - val_loss: 2.7304\n",
      "Epoch 47/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.1587 - val_loss: 2.7012\n",
      "Epoch 48/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.9904 - val_loss: 2.6828\n",
      "Epoch 49/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.8882 - val_loss: 2.6577\n",
      "Epoch 50/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.8060 - val_loss: 2.5695\n",
      "Epoch 51/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.7216 - val_loss: 2.5345\n",
      "Epoch 52/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.7609 - val_loss: 2.4949\n",
      "Epoch 53/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.6385 - val_loss: 2.4555\n",
      "Epoch 54/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.5936 - val_loss: 2.3846\n",
      "Epoch 55/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.5510 - val_loss: 2.3364\n",
      "Epoch 56/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.5364 - val_loss: 2.3245\n",
      "Epoch 57/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.3475 - val_loss: 2.3167\n",
      "Epoch 58/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.4740 - val_loss: 2.2618\n",
      "Epoch 59/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.2870 - val_loss: 2.2482\n",
      "Epoch 60/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.2779 - val_loss: 2.2341\n",
      "Epoch 61/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.2485 - val_loss: 2.1927\n",
      "Epoch 62/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.1651 - val_loss: 2.1462\n",
      "Epoch 63/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.1129 - val_loss: 2.1843\n",
      "Epoch 64/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.0260 - val_loss: 2.1459\n",
      "Epoch 65/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5.0367 - val_loss: 2.0694\n",
      "Epoch 66/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.9643 - val_loss: 2.0752\n",
      "Epoch 67/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9478 - val_loss: 2.0926\n",
      "Epoch 68/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.9095 - val_loss: 2.0549\n",
      "Epoch 69/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.8655 - val_loss: 2.0129\n",
      "Epoch 70/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.7748 - val_loss: 2.0903\n",
      "Epoch 71/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.7967 - val_loss: 2.0566\n",
      "Epoch 72/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.7176 - val_loss: 1.9957\n",
      "Epoch 73/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.7222 - val_loss: 1.9875\n",
      "Epoch 74/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.6613 - val_loss: 2.0176\n",
      "Epoch 75/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.7274 - val_loss: 1.9143\n",
      "Epoch 76/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.5360 - val_loss: 1.9223\n",
      "Epoch 77/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.5717 - val_loss: 1.9781\n",
      "Epoch 78/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.6022 - val_loss: 1.9141\n",
      "Epoch 79/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5293 - val_loss: 1.8742\n",
      "Epoch 80/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.4242 - val_loss: 1.9770\n",
      "Epoch 81/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.4198 - val_loss: 1.9844\n",
      "Epoch 82/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 4.4185 - val_loss: 1.8579\n",
      "Epoch 83/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.3414 - val_loss: 1.8354\n",
      "Epoch 84/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.3766 - val_loss: 1.8900\n",
      "Epoch 85/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.3440 - val_loss: 1.8902\n",
      "Epoch 86/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.3223 - val_loss: 1.8326\n",
      "Epoch 87/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.2599 - val_loss: 1.8571\n",
      "Epoch 88/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.2222 - val_loss: 1.8335\n",
      "Epoch 89/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.2037 - val_loss: 1.8288\n",
      "Epoch 90/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.1539 - val_loss: 1.8441\n",
      "Epoch 91/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.2088 - val_loss: 1.7827\n",
      "Epoch 92/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.1609 - val_loss: 1.7586\n",
      "Epoch 93/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.1128 - val_loss: 1.8320\n",
      "Epoch 94/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.1576 - val_loss: 1.8213\n",
      "Epoch 95/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.0132 - val_loss: 1.7439\n",
      "Epoch 96/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.0100 - val_loss: 1.7751\n",
      "Epoch 97/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.9618 - val_loss: 1.8559\n",
      "Epoch 98/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.0082 - val_loss: 1.7459\n",
      "Epoch 99/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.9401 - val_loss: 1.7430\n",
      "Epoch 100/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.0409 - val_loss: 1.7562\n",
      "Epoch 101/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.9485 - val_loss: 1.7098\n",
      "Epoch 102/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.8384 - val_loss: 1.7247\n",
      "Epoch 103/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.8137 - val_loss: 1.8583\n",
      "Epoch 104/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.8858 - val_loss: 1.7341\n",
      "Epoch 105/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.8819 - val_loss: 1.6045\n",
      "Epoch 106/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.8628 - val_loss: 1.7953\n",
      "Epoch 107/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.8452 - val_loss: 1.8263\n",
      "Epoch 108/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.8112 - val_loss: 1.6520\n",
      "Epoch 109/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.7681 - val_loss: 1.6561\n",
      "Epoch 110/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.7214 - val_loss: 1.7920\n",
      "Stopped fitting after 110 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk in /var/folders/t1/fw_bh5nj2sg1zsn8ft2cld2c0000gn/T/tmpy1yo7oy8\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepimpute.multinet.MultiNet at 0x296732710>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom fit\n",
    "trainingData = data.iloc[100:250,:]\n",
    "multinet.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "\n",
    "The imputation can be done on any dataset as long as the gene labels are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Filling zeros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juank/Desktop/BCOM/Proyecto/deepimpute/deepimpute/multinet.py:287: FutureWarning: DataFrame.groupby with axis=1 is deprecated. Do `frame.T.groupby(...)` without axis instead.\n",
      "  predicted = predicted.groupby(by=predicted.columns, axis=1).mean()\n"
     ]
    }
   ],
   "source": [
    "imputedData = multinet.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "limits = [0,100]\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "jitter = np.random.normal(0,1,data.size) # Add some jittering to better see the point density\n",
    "ax.scatter(data.values.flatten()+jitter,imputedData.values.flatten(),s=2)\n",
    "ax.plot(limits,limits,'r-.',linewidth=2)\n",
    "ax.set_xlim(limits)\n",
    "ax.set_ylim(limits)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "Display training metrics (MSE and Pearson's correlation on the test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'correlation': 0.8939244665425083, 'MSE': 0.19052126079416265}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinet.test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
